{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK for Python Test Bed\n",
    "## Chapter 3:  Processing Raw Text\n",
    "### Using examples from http://www.nltk.org\n",
    "##### Whitney King  (6/5/2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display, HTML\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "import pprint as pp\n",
    "import datetime as dt\n",
    "import emoji\n",
    "import re\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define custom functions\n",
    "\n",
    "def line():\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Accessing Text from the Web and from Disk_\n",
    "\n",
    "### _Electronic Books_\n",
    "\n",
    "Resources such as *Project Gutenberg* have text versions of books online for free. There are over 25,000 to choose from on the PG website, in over 50 languages, all of which can be downloaded in ASCII."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74726  |  <class 'str'>  |  The Project Gutenberg EBook of Alice in Wonderland, by Lewis Carroll\r\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'list'> 15758 ['The', 'Project', 'Gutenberg', 'EBook', 'of', 'Alice', 'in', 'Wonderland', ',', 'by']\n"
     ]
    }
   ],
   "source": [
    "from urllib import request\n",
    "url = \"http://www.gutenberg.org/files/19033/19033.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "print(len(raw), ' | ', type(raw), ' | ',  raw[:69])\n",
    "line()\n",
    "tokens = word_tokenize(raw)\n",
    "print(type(tokens), len(tokens), tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a lot of descriptive information about the book is appearing in the collocations, they should be trimmed from the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.text.Text'>\n",
      "['alas',\n",
      " '!',\n",
      " 'either',\n",
      " 'the',\n",
      " 'locks',\n",
      " 'were',\n",
      " 'too',\n",
      " 'large',\n",
      " ',',\n",
      " 'or',\n",
      " 'the',\n",
      " 'key',\n",
      " 'was',\n",
      " 'too',\n",
      " 'small']\n",
      "Project Gutenberg-tm; Project Gutenberg; said Alice; Literary Archive;\n",
      "White Rabbit; Archive Foundation; Gutenberg-tm electronic; Gutenberg\n",
      "Literary; electronic works; United States; March Hare; public domain;\n",
      "set forth; golden key; electronic work; white kid-gloves; Gutenberg-tm\n",
      "License; play croquet; Mary Ann; thought Alice\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "text = nltk.Text(tokens)\n",
    "print(type(text))\n",
    "pp.pprint(text[976:991])\n",
    "pp.pprint(text.collocations())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = raw[raw.find(\"I--DOWN THE RABBIT-HOLE\"):raw.rfind(\"End of the Project Gutenberg EBook\")]\n",
    "\n",
    "raw.find(\"I--DOWN THE RABBIT-HOLE\") #Trimmed book text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said Alice; White Rabbit; March Hare; golden key; white kid-gloves;\n",
      "play croquet; Mary Ann; thought Alice; inches high; little golden;\n",
      "feet high; cool fountains; yer honor; good deal; low voice; asking\n",
      "riddles; right size; trembling voice; shrinking rapidly; came upon\n",
      "said Alice; White Rabbit; March Hare; golden key; white kid-gloves;\n",
      "play croquet; Mary Ann; thought Alice; inches high; little golden;\n",
      "feet high; cool fountains; yer honor; good deal; low voice; asking\n",
      "riddles; right size; trembling voice; shrinking rapidly; came upon\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(raw)\n",
    "text = nltk.Text(tokens)\n",
    "text.collocations()\n",
    "pp.pprint(text.collocations())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _HTML_\n",
    "\n",
    "Packages such as BeautifulSoup enable the ability to wrange text from HTML web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html>\n",
      "<html lang=\"en\" dir=\"ltr\" class=\"\">\n",
      "<head>\n",
      "\n",
      "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n",
      "\t<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, user-scalable=yes\">\n",
      "<meta name=\"generator\" content=\"MediaWiki 1.19.24\" />\n",
      "<meta name=\"keywords\" content=\"Sailor Moon Wiki,sailormoon,Sailor Galaxia,Act 50 - Stars 1,Chaos,Sailor Chaos,Sailor Chi and Sailor Phi,Sailor Lethe,Sailor Mnemosyne,Sailor Iron Mouse,Sailor Aluminum Siren,Sailor Lead Crow,Sailor Tin Nyanko\" />\n",
      "<meta name=\"description\" content=\"Sailor Galaxia is one of the main antagonists in the final arc of Sailor Moon. She is well known amongst the Galaxy for wreaking havoc and ruining worlds in her quest to obtain the strongest Senshi Crystal. She is the most powerful Sailor Senshi in the galaxy and the ruler of Shadow Galactica...\" />\n",
      "<meta name=\"twitter:card\" content=\"summary\" />\n",
      "<meta name=\"twitter:site\" content=\"@getfandom\" />\n",
      "<meta name=\"twitter:url\" content=\"http://sailormoo\n",
      "--------------------------------------------------------------------------------\n",
      "['aliases', 'alien', 'amazon', 'amongst', 'amount', 'base', 'become', 'behavior', 'beings', 'built', 'cache', 'calm', 'castle', 'category', 'cause', 'chibi', 'collected', 'contents', 'context', 'cosmic', 'counterpart', 'crown', 'data', 'death', 'destroying', 'destruction', 'done', 'dress', 'eternity', 'existed', 'existence', 'fairies', 'feet', 'head', 'kaolinite', 'kingdom', 'knew', 'known', 'learned', 'lipstick', 'loader', 'looked', 'made', 'main', 'makes', 'moon', 'most', 'mostly', 'mouse', 'musicals', 'nabu', 'nothingness', 'objective', 'occupation', 'oppositio', 'optionally', 'plans', 'prod', 'production', 'random', 'reaches', 'referred', 'remove', 'resurrecting', 'russian', 'search', 'seen', 'shadow', 'shingo', 'sign', 'start', 'strike', 'talk', 'thoughts', 'tomoe', 'trackingoptin', 'undefined', 'unlike', 'vandalism', 'vast', 'well', 'went', 'wgaddriveraolbiddercountries', 'wgaddriverfvdelaycountries', 'wgaddriverincontentplayerslotcountries', 'wgaddriverkilocountries', 'wgaddrivermoattrackingforfeaturedvideoadcountries', 'wgaddrivermobilenivensrabbitcountries', 'wgaddriverplayadsonnextfvfrequency', 'wgaddriverrubiconprebidcountries', 'wgarticlevideonextvideoautoplaycountries', 'wgenabletrackingoptinmodal', 'where', 'wikicategory', 'with', 'worlds', 'worthy', 'would', 'wrong', 'セーラーギャラクシアsērā']\n",
      "--------------------------------------------------------------------------------\n",
      "Displaying 25 of 139 matches:\n",
      "                                    sailor moon well known amongst galaxy wrea\n",
      "ongest senshi crystal most powerful sailor senshi galaxy ruler shadow galactic\n",
      "enshi galaxy ruler shadow galactica sailor galaxia sailor moon wiki fandom pow\n",
      "ler shadow galactica sailor galaxia sailor moon wiki fandom powered wikia lang\n",
      "s https sailormoon shadow galactica sailor senshi villains manga characters fe\n",
      "ters female manga biographies manga sailor senshi manga villains manga article\n",
      "t register start wiki advertisement sailor moon wiki pages page media manga co\n",
      "iki pages page media manga codename sailor pretty soldier sailor moon bssm cha\n",
      "anga codename sailor pretty soldier sailor moon bssm chapter guide manga volum\n",
      "m chapter guide manga volumes anime sailor moon sailor moon sailor moon sailor\n",
      "ide manga volumes anime sailor moon sailor moon sailor moon sailor moon supers\n",
      "lumes anime sailor moon sailor moon sailor moon sailor moon supers sailor moon\n",
      "sailor moon sailor moon sailor moon sailor moon supers sailor moon sailor star\n",
      "moon sailor moon sailor moon supers sailor moon sailor stars pretty guardian s\n",
      "moon sailor moon supers sailor moon sailor stars pretty guardian sailor moon c\n",
      "r moon sailor stars pretty guardian sailor moon crystal films specials musical\n",
      "cials musicals sera pretty guardian sailor moon reconquista pretty soldier sai\n",
      "lor moon reconquista pretty soldier sailor moon alternate legend dark kingdom \n",
      "ingdom revival story pretty soldier sailor moon usagi path become warrior love\n",
      " become warrior love pretty soldier sailor moon supers revision dream warriors\n",
      "turn revival chapter pretty soldier sailor moon infinity academy mistress laby\n",
      "y mistress labyrinth pretty soldier sailor moon sailor stars revision series v\n",
      "abyrinth pretty soldier sailor moon sailor stars revision series video games b\n",
      " series video games bishoujo senshi sailor moon another story oppositio senshi\n",
      "shi ishtar nabu nergal marduk music sailor star song power love moonlight lege\n",
      "--------------------------------------------------------------------------------\n",
      "sailor moon; shadow galactica; pretty soldier; lang lang; require\n",
      "function; trackingoptin function; solar system; ultimate force; force\n",
      "destruction; chibi moon; sailor crystals; function loadscript;\n",
      "trackingoptout trackingoptin; havoc ruining; main antagonists; ruining\n",
      "worlds; super string; wreaking havoc; sailor mars; sailor mercury\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "url = \"http://sailormoon.wikia.com/wiki/Sailor_Galaxia\"\n",
    "html = request.urlopen(url).read().decode('utf8')\n",
    "print(html[:1000])\n",
    "line()\n",
    "\n",
    "html_raw = BeautifulSoup(html[554:126566], \"lxml\")\n",
    "tokens = word_tokenize(html_raw.get_text())\n",
    "print(sorted(list(set([w.lower() for w in tokens if w.isalpha() and len(w) > 3]))[:100]))\n",
    "line()\n",
    "\n",
    "html_text = nltk.Text([w.lower() for w in tokens if w.isalpha() and len(w) > 3])\n",
    "html_text.concordance('sailor')\n",
    "line()\n",
    "\n",
    "html_text.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Engine Results\n",
    "\n",
    "While a pretty rudimentary method, hits for collocations of words via a search engine such a Google can provide fairly enlightening information. Web search engines provide an efficient means of searching this large quantity of text for relevant linguistic examples. Unlike local corpora, where you write programs to search for arbitrarily complex patterns, search engines generally only allow you to search for individual words or strings of words, sometimes with wildcards. Overall, they are a handy tool for performing quick checks on a theory.\n",
    "\n",
    "**Google Hits for Collocations:** \n",
    "\n",
    "    Number of hits for collocations involving absolutely or definitely, followed by one of adore, love, like, or prefer\n",
    "\n",
    "|Google hits | adore | love | like | prefer |\n",
    "|-----|-----|-----|-----|-----|\n",
    "|absolutely | 289,000 | 905,000 | 16,200 | 644|\n",
    "|definitely | 1,460 | 51,000\t | 58,000 | 62,600|\n",
    "|ratio | 198:1 | 18:1 | 1:10 | 1:97|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RSS Feeds\n",
    "\n",
    "On the modern web, blogs are an important vecotr for individual voices to be heard, both formally and informally. Blogs are published as RSS feeds, which can be used as a souce of text. The Python library ```feedparser``` can be used to access the content of a blogs RSS feed, which can then be munged and analyzed using various other libraries.\n",
    "\n",
    "Most blogs use the same schema, which can then be broken down to get the text parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ars Technica | Retrieved: 2018-06-07 18:56:54 PST\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "[1]&nbsp;&nbsp;<a href=\"https://arstechnica.com/?p=1323177\" target=\"_blank\">Facebook privacy goof makes posts by 14 million users readable to anyone</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[2]&nbsp;&nbsp;<a href=\"https://arstechnica.com/?p=1323159\" target=\"_blank\">Join us at 8:30 pm ET today for live-streamed games ahead of E3 madness</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[3]&nbsp;&nbsp;<a href=\"https://arstechnica.com/?p=1323113\" target=\"_blank\">Microsoft’s plan for GitHub: “Make GitHub better at being GitHub”</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[4]&nbsp;&nbsp;<a href=\"https://arstechnica.com/?p=1323093\" target=\"_blank\">Ubisoft CEO: Cloud gaming will replace consoles after the next generation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[5]&nbsp;&nbsp;<a href=\"https://arstechnica.com/?p=1322927\" target=\"_blank\">Dealmaster: Get a pair of Anker wireless exercise headphones for $30</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[6]&nbsp;&nbsp;<a href=\"https://arstechnica.com/?p=1322989\" target=\"_blank\">Stymied by browsers, attackers embed Flash 0-day inside MS Office document</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[7]&nbsp;&nbsp;<a href=\"https://arstechnica.com/?p=1322933\" target=\"_blank\">Training a neural network in phase-change memory beats GPUs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[8]&nbsp;&nbsp;<a href=\"https://arstechnica.com/?p=1322937\" target=\"_blank\">Net neutrality will be repealed Monday unless Congress takes action</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[9]&nbsp;&nbsp;<a href=\"https://arstechnica.com/?p=1322279\" target=\"_blank\">Finally, scientists have found intriguing organic molecules on Mars</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[10]&nbsp;&nbsp;<a href=\"https://arstechnica.com/?p=1322717\" target=\"_blank\">Latest Windows preview suggests you’ll be able to turn S Mode on and off</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[11]&nbsp;&nbsp;<a href=\"https://arstechnica.com/?p=1322439\" target=\"_blank\">Android P Preview 3 hands-on—Here’s all 157 new emoji, some UI tweaks</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[12]&nbsp;&nbsp;<a href=\"https://arstechnica.com/?p=1322729\" target=\"_blank\">Machines that suck CO₂ from the air might be cheaper than we thought</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[13]&nbsp;&nbsp;<a href=\"https://arstechnica.com/?p=1322719\" target=\"_blank\">Op-ed: Valve takes a side by not “taking sides” in curation controversy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[14]&nbsp;&nbsp;<a href=\"https://arstechnica.com/?p=1322761\" target=\"_blank\">New T-Mobile upgrade may boost your coverage—if you have the right phone</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[15]&nbsp;&nbsp;<a href=\"https://arstechnica.com/?p=1322645\" target=\"_blank\">NTSB: Autopilot steered Tesla car toward traffic barrier before deadly crash</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[16]&nbsp;&nbsp;<a href=\"https://arstechnica.com/?p=1322651\" target=\"_blank\">More Americans in China say they’re victims of mysterious health attacks</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[17]&nbsp;&nbsp;<a href=\"https://arstechnica.com/?p=1322613\" target=\"_blank\">New law forces Google to suspend political ads in Washington state</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[18]&nbsp;&nbsp;<a href=\"https://arstechnica.com/?p=1322511\" target=\"_blank\">Mueller checks witnesses’ phones for secure messaging apps, per report</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[19]&nbsp;&nbsp;<a href=\"https://arstechnica.com/?p=1321827\" target=\"_blank\">Confirmed: ZTE to reopen after $1 billion fine, new leadership [Updated]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[20]&nbsp;&nbsp;<a href=\"https://arstechnica.com/?p=1322403\" target=\"_blank\">Amazon’s Fire TV Cube is like a Fire TV blended with an Echo Dot</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import feedparser as fp\n",
    "\n",
    "# ArsTechnia RSS Feed\n",
    "get_time = dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "llog = fp.parse('http://feeds.arstechnica.com/arstechnica/index')\n",
    "\n",
    "print('{0} | Retrieved: {1} PST'.format(llog.feed.title, get_time))\n",
    "line()\n",
    "\n",
    "for i in range(0, len(llog.entries)):\n",
    "    entry = llog.entries[i]\n",
    "    display(HTML('[{0}]&nbsp;&nbsp;<a href=\"{1}\" target=\"_blank\">{2}</a>'\\\n",
    "                 .format(i+1, entry.link, entry.title)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finally, scientists have found intriguing organic molecules on Mars\n",
      "--------------------------------------------------------------------------------\n",
      "['Enlarge', '/', 'Since', '2012', ',', 'NASA', \"'s\", 'Curiosity', 'rover', 'has', 'been', 'trying', 'to', 'find', 'organic', 'molecules', '.', 'Now', ',', 'it', 'has', 'succeeded', '.', '(', 'credit', ':', 'NASA', ')', 'After', 'more', 'than', 'four', 'decades', 'of', 'searching', 'for', 'organic', 'molecules', 'on', 'the', 'surface', 'of', 'Mars', ',', 'scientists', 'have', 'conclusively', 'found', 'them', 'in', 'mudstones', 'on', 'the', 'lower', 'slopes', 'of', 'Mount', 'Sharp', '.', 'A', 'variety', 'of', 'organic', 'compounds', 'were', 'discovered', 'by', 'NASA', \"'s\", 'Curiosity', 'rover', ',', 'which', 'heated', 'the', 'Martian', 'rocks', 'to', '500°', 'Celsius', 'to', 'release', 'the', 'chemicals', '.', 'The', 'finding', 'is', 'significant—for', 'life', 'to', 'have', 'ever', 'existed', 'on', 'Mars', 'there', 'would', 'almost', 'certainly', 'need', 'to', 'be', 'organic', 'molecules', 'to', 'get', 'it', 'started', ';', 'they', \"'re\", 'the', 'basic', 'building', 'blocks', 'of', 'life', 'as', 'we', 'know', 'it', '.', 'And', 'if', 'life', 'did', 'get', 'started', ',', 'it', 'would', 'have', 'left', 'organic', 'molecules', 'behind', '.', 'However', 'the', 'confirmation', 'of', 'organics', 'on', 'Mars', 'raises', 'more', 'questions', 'than', 'it', 'answers', '.', 'Based', 'upon', 'the', 'information', 'scientists', 'have', 'gleaned', 'so', 'far', ',', 'they', 'can', 'not', 'determine', 'whether', 'these', 'organics', 'were', 'produced', 'by', 'life', ',', 'delivered', 'to', 'the', 'surface', 'of', 'Mars', 'by', 'meteorites', ',', 'or', 'are', 'the', 'byproduct', 'of', 'geological', 'processes', 'on', 'Mars', '.', 'The', 'Viking', 'landers', 'reached', 'the', 'surface', 'of', 'Mars', 'during', 'the', 'summer', 'of', '1976', 'amid', 'some', 'expectation', 'that', 'they', 'might', 'find', 'evidence', 'of', 'past', 'life', ',', 'if', 'not', 'life', 'itself', '.', 'However', ',', 'when', 'Viking', 'landers', 'sampled', 'the', 'Martian', 'soil', 'they', 'found', 'no', 'past', 'life', ',', 'nor', 'did', 'their', 'gas', 'chromatograph', 'mass', 'spectrometers', 'find', 'any', 'organic', 'molecules', '.', 'Nada', '.', 'Read', '11', 'remaining', 'paragraphs', '|', 'Comments']\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Dig down into a post\n",
    "post = llog.entries[8]\n",
    "print(post.title)\n",
    "line()\n",
    "\n",
    "# Get post content\n",
    "content = post.content[0].value\n",
    "raw_content = BeautifulSoup(content).get_text()\n",
    "print(word_tokenize(raw_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Reading Local Files_\n",
    "\n",
    "In addition to the ```open()``` function built into Python that read flat files, third party packages such as ```pypdf``` and ```pywin32``` will read PDF and Word files.\n",
    "\n",
    "* Corpus files can also be read in this same manner\n",
    "* Any string, including ```input()``` can be tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'Pokemon.txt'\n",
    "with open(f, 'r') as fraw:\n",
    "    line = fraw.readline()\n",
    "    while line:\n",
    "        #print(\"{}\".format(line.strip()))\n",
    "        line = fraw.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Unicode\n",
    "\n",
    "Unicode is a method for encoding characters to information that supports over 1 mioon characters. The ```open()``` function can read encoded data into Unicode strings, and write out Unicode strings in encoded form. Each character is assigned a number called **code point**, which takes on the form of ```\\uXXXX```, where ```XXXX``` is a 4 digit hexadecimal number. From a Unicode perspective, characters are abstract entities which can be realized as one or more **glyphs**. Glyphs can appear on a screen or be printed on paper. A font is a mapping from characters to glyphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are not working in the emoji module directly, even with modifications.\n",
    "# Investigate further; they are directly from the module\n",
    "\n",
    "def emoji_lis(string):\n",
    "    \"\"\"Return the location and emoji in list of dic format\n",
    "    >>>emoji.emoji_lis(\"Hi, I am fine. 😁\")\n",
    "    >>>[{'location': 15, 'emoji': '😁'}]\n",
    "    \"\"\"\n",
    "    _entities = []\n",
    "    for pos,c in enumerate(string):\n",
    "        if c in emoji.UNICODE_EMOJI:\n",
    "            _entities.append({\n",
    "                \"location\":pos,\n",
    "                \"emoji\": c\n",
    "                })\n",
    "    return _entities\n",
    "\n",
    "def emoji_count(string):\n",
    "    \"\"\"Returns the count of emojis in a string\"\"\"\n",
    "    c=0\n",
    "    for i in string:\n",
    "        if i in emoji.UNICODE_EMOJI:\n",
    "            c=c+1\n",
    "    return(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://unicode.org/Public/emoji/11.0/ for updated list of codes\n",
    "emoji_pattern = emoji.get_emoji_regexp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['🦄', '🌮', '😎', '👾', '🖖', '💞']\n"
     ]
    }
   ],
   "source": [
    "# 😎\\U1F60E 👾\\U1F47E 🖖\\U1F596 💞\\U1F49E 🦄\\U1F984 🌮\\U1F32E\n",
    "\n",
    "text = u\"🦄 This text is rocking 🌮 some pretty sweet emojis 😎 👾 🖖 💞\"\n",
    "\n",
    "find_emoji = emoji_pattern.findall(text)\n",
    "print(find_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__unicorn_face__ This text is rocking __taco__ some pretty sweet emojis __smiling_face_with_sunglasses__ __alien_monster__ __vulcan_salute__ __revolving_hearts__\n"
     ]
    }
   ],
   "source": [
    "# Currently replaces with English, but could be extended to replace with Unicode\n",
    "demoji = emoji.demojize(text, delimiters=('__','__'))\n",
    "print(demoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'emoji': '🦄', 'location': 0},\n",
      " {'emoji': '🌮', 'location': 23},\n",
      " {'emoji': '😎', 'location': 50},\n",
      " {'emoji': '👾', 'location': 52},\n",
      " {'emoji': '🖖', 'location': 54},\n",
      " {'emoji': '💞', 'location': 56}]\n",
      "--------------------------------------------------------------------------------\n",
      "Emojis:  6\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(emoji_lis(text))\n",
    "line()\n",
    "print('Emojis: ', emoji_count(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module ```unicodedata``` lets us inspect the properties of Unicode characters. We can use this to get the unicode values of the emojis, as well as the other hardcoded properties, or we can encode the tag in other ways. Either way, there is a weath of possibility for how emojis can be used in text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xf0\\x9f\\xa6\\x84' U+1f984 UNICORN FACE\n",
      "b'\\xf0\\x9f\\x8c\\xae' U+1f32e TACO\n",
      "b'\\xf0\\x9f\\x98\\x8e' U+1f60e SMILING FACE WITH SUNGLASSES\n",
      "b'\\xf0\\x9f\\x91\\xbe' U+1f47e ALIEN MONSTER\n",
      "b'\\xf0\\x9f\\x96\\x96' U+1f596 RAISED HAND WITH PART BETWEEN MIDDLE AND RING FINGERS\n",
      "b'\\xf0\\x9f\\x92\\x9e' U+1f49e REVOLVING HEARTS\n"
     ]
    }
   ],
   "source": [
    "for c in text:\n",
    "    if ord(c) > 127: # Skips single characters\n",
    "        print('{} U+{:04x} {}'.format(c.encode('utf8'), ord(c), unicodedata.name(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\\\U0001f984 This text is rocking \\\\U0001f32e some pretty sweet emojis \\\\U0001f60e \\\\U0001f47e \\\\U0001f596 \\\\U0001f49e'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicode = text.encode('unicode_escape')\n",
    "print(unicode)\n",
    "unicode.find(b'\\U0001f60e')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing Text\n",
    "\n",
    "When we tokenize a string we produce a list (of words), and this is Python's ```<list>``` type. Strings and lists are both kinds of sequence. We can pull them apart by indexing and slicing them, and we can join them together by concatenating them. However, we cannot join strings and lists.\n",
    "\n",
    "Now that you can present emoji (or any other glyph) data in your desired format, you can tokenize in in whichever format you find more suitable to your needs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__unicorn_face__', 'this', 'text', 'is', 'rocking', '__taco__', 'some', 'pretty', 'sweet', 'emojis', '__smiling_face_with_sunglasses__', '__alien_monster__', '__vulcan_salute__', '__revolving_hearts__']\n"
     ]
    }
   ],
   "source": [
    "# Unicode tokens\n",
    "unicode_tokens = word_tokenize(demoji.lower())\n",
    "print(unicode_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\\\u0001f984', 'this', 'text', 'is', 'rocking', '\\\\u0001f32e', 'some', 'pretty', 'sweet', 'emojis', '\\\\u0001f60e', '\\\\u0001f47e', '\\\\u0001f596', '\\\\u0001f49e']\n"
     ]
    }
   ],
   "source": [
    "# Shortcode tokens\n",
    "alias_tokens = word_tokenize(unicode.decode('utf-8').lower())\n",
    "print(alias_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['🦄', 'this', 'text', 'is', 'rocking', '🌮', 'some', 'pretty', 'sweet', 'emojis', '😎', '👾', '🖖', '💞']\n"
     ]
    }
   ],
   "source": [
    "# Emoji tokens\n",
    "emoji_tokens = word_tokenize(text.lower())\n",
    "print(emoji_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NLPPipeline](http://whitneyontheweb.com/images/pipeline1.png \"NLP Pipeline\")\n",
    "\n",
    "|```Method``` | Functionality | \n",
    "|-----|-----|-----|\n",
    "|```s.find(t)``` | index of first instance of string t inside s (-1 if not found) | \n",
    "|```s.rfind(t)``` | index of last instance of string t inside s (-1 if not found) | \n",
    "|```s.index(t)``` | like s.find(t) except it raises ValueError if not found | \n",
    "|```s.rindex(t)``` | like s.rfind(t) except it raises ValueError if not found | \n",
    "|```s.join(text)``` | combine the words of the text into a string using s as the glue | \n",
    "|```s.split(t)``` | split s into a list wherever a t is found (whitespace by default) | \n",
    "|```s.splitlines()``` | split s into a list of strings, one per line | \n",
    "|```s.lower()``` | a lowercased version of the string s | \n",
    "|```s.upper()``` | an uppercased version of the string s | \n",
    "|```s.title()``` | a titlecased version of the string s | \n",
    "|```s.strip()``` | a copy of s without leading or trailing whitespace | \n",
    "|```s.replace(t, u)``` | replace instances of t with u inside s | \n",
    "\n",
    "### _Regular Expressions_\n",
    "\n",
    "Regexes are pattern matching expressions. Regexes can be used to extract for many things; examples include extracting date parts in the desired format from text, or locating words that end in 'ed' or 'ing', or words that have certain characters spliced throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2009, 12, 31]\n",
      "['adreamed', 'adscripted', 'aduncated', 'advanced', 'advised', 'aeried', 'aethered', 'afeared', 'affected', 'affectioned']\n",
      "['aging', 'agoing', 'agreeing', 'ailing', 'aiming', 'airing', 'aisling', 'alarming', 'allthing', 'alluring']\n",
      "['abjectly', 'adjuster', 'dejected', 'dejectly', 'injector', 'majestic', 'objectee', 'objector', 'rejecter', 'rejector']\n"
     ]
    }
   ],
   "source": [
    "wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]\n",
    "\n",
    "# Date Parts\n",
    "print([int(n) for n in re.findall(r'\\d+', '2009-12-31')])\n",
    "\n",
    "# Words ending in -ed\n",
    "ed_words = [w for w in wordlist if re.search('ed$', w)]\n",
    "print(ed_words[50:60])\n",
    "\n",
    "# Words ending in -ed\n",
    "ing_words = [w for w in wordlist if re.search('ing$', w)]\n",
    "print(ing_words[30:40])\n",
    "\n",
    "# Words with ..j..t.. (8 letters, 2 rand, j, 2 rand, t, 2 rand) \n",
    "# ^ matches start of string, $ matches end limits results to 8 characters\n",
    "jt_words = [w for w in wordlist if re.search('^..j..t..$', w)]\n",
    "print(jt_words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expressions are great for extracting parts of words, amoung other things.\n",
    "\n",
    "|```Operator``` | Behavior | \n",
    "|-----|-----|-----|\n",
    "|```.``` | Wildcard, matches any character | \n",
    "|```^abc``` | Matches some pattern abc at the start of a string | \n",
    "|```abc$``` | Matches some pattern abc at the end of a string | \n",
    "|```[abc]``` | Matches one of a set of characters | \n",
    "|```[A-Z0-9]``` | Matches one of a range of characters | \n",
    "|```ed\\ing\\s``` | Matches one of the specified strings (disjunction) | \n",
    "|```*``` | Zero or more of previous item, e.g. a*, [a-z]* (also known as Kleene Closure) | \n",
    "|```+``` | One or more of previous item, e.g. a+, [a-z]+ | \n",
    "|```?``` | Zero or one of the previous item (i.e. optional), e.g. a?, [a-z]? | \n",
    "|```{n}``` | Exactly n repeats where n is a non-negative integer | \n",
    "|```{n,}``` | At least n repeats | \n",
    "|```{,n}``` | No more than n repeats | \n",
    "|```{m,n}``` | At least m and no more than n repeats | \n",
    "|```a(b\\c)+``` | Parentheses that indicate the scope of the operators | \n",
    "\n",
    "The T9 system is used for entering text on mobile phones\n",
    "\n",
    "* Two or more words that are entered with the same sequence of keystrokes are known as **textonyms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ace', 'bad', 'bae', 'cad']\n",
      "['paced', 'scaff']\n",
      "['g', 'ghoom', 'gig', 'giggling', 'gigolo', 'gilim', 'gill', 'gilling', 'gilo', 'gim', 'gin', 'ging', 'gingili', 'gink', 'ginkgo', 'ginning', 'gio', 'glink', 'glom', 'glonoin']\n",
      "['able', 'abloom', 'abode', 'abolla', 'aboma', 'aboon', 'academe', 'acana', 'acca', 'accede', 'accedence', 'accend', 'accolade', 'accoladed', 'accolle', 'accommodable', 'ace', 'ackman', 'acle', 'acme']\n"
     ]
    }
   ],
   "source": [
    "# 3 letter textonyms\n",
    "print([w for w in wordlist if re.search('^[abc][abc][def]$', w)])\n",
    "\n",
    "# 5 letter textonyms\n",
    "print([w for w in wordlist if re.search('^[pqrs][abc][abc][def][def]$', w)])\n",
    "\n",
    "# \"finger-twisters\"\n",
    "# words that only use part of the number-pad. \n",
    "# For example «^[ghijklmno]+$», or more concisely, «^[g-o]+$», \n",
    "# will match words that only use keys 4, 5, 6 in the center row, \n",
    "# and «^[a-fj-o]+$» will match words that use keys 2, 3, 5, 6 in the top-right corner\n",
    "ft_g_o = [w for w in wordlist if re.search('^[g-o]+$', w)]\n",
    "ft_a_fj_o = [w for w in wordlist if re.search('^[a-fj-o]+$', w)]\n",
    "print(ft_g_o[:20])\n",
    "print(ft_a_fj_o[20:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what did the special characters accomplish for us in that last regex? \n",
    "-  ```+``` simply means \"one or more instances of the preceding item\". \n",
    "-  If we replace ```+``` with ```*```, it means \"zero or more instances of the preceding item\"\n",
    "- Note that the + and * symbols are sometimes referred to as **Kleene closures**, or simply **closures**\n",
    "- ^ inside of square brackets [^ed] matches any charcters EXCEPT those included in the bracket with it.\n",
    "\n",
    "If we explore these more, we can see how they behave when applied in different contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chat_words = sorted(set(w for w in nltk.corpus.nps_chat.words()))\n",
    "\n",
    "chat_posts = []\n",
    "for post in nltk.corpus.nps_chat.posts():\n",
    "    jp = ' '.join([w for w in post])\n",
    "    chat_posts.append(jp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['miiiiiiiiiiiiinnnnnnnnnnneeeeeeeeee', 'miiiiiinnnnnnnnnneeeeeeee', 'mine', 'mmmmmmmmiiiiiiiiinnnnnnnnneeeeeeee']\n"
     ]
    }
   ],
   "source": [
    "# One or more of each of the letters annotated with +\n",
    "mine = [w for w in chat_words if re.search('^m+i+n+e+$', w)]\n",
    "print(mine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'aaaaaaaaaaaaaaaaa', 'aaahhhh', 'ah', 'ahah', 'ahahah', 'ahh', 'ahhahahaha', 'ahhh', 'ahhhh']\n"
     ]
    }
   ],
   "source": [
    "# One or more of either of the bracketed characters in the set annotated with +\n",
    "h_or_a = [w for w in chat_words if re.search('^[ha]+$', w)]\n",
    "print(h_or_a[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['99', '99701', '99703', '9:10', ':', ':(', ':)', ':):):)', ':-(', ':-)', ':-@', ':.', ':/', ':@', ':D', ':P', ':]', ':p', ':|', ';', '; ..', ';)', ';-(', ';-)', ';0', ';]', ';p', '<', '<,', '<-', '<--', '<---', '<----', '<----------', '<3', \"<3's\", '<33', '<333', '<3333', '<33333']\n"
     ]
    }
   ],
   "source": [
    "# Tokens with all non-vowel characters\n",
    "non_vowels = [w for w in chat_words if re.search('^[^aeiouAEIOU]+$', w)]\n",
    "print(non_vowels[305:345])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.98', '1.99', '102.6', '121.7', '147.7', '2.3', '39.3', '4.20', '45.5', '64.8', '9.53', '98.5', '98.6']\n"
     ]
    }
   ],
   "source": [
    "# Tokens that are decimal numbers\n",
    "print([w for w in chat_words if re.search('^[0-9]+\\.[0-9]+$', w)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Diary', 'Did', 'Diego', 'Dipset', 'Dixie', 'Do', 'Does', 'Doing', 'Dokken', 'Dolls', 'Dood', 'Down', 'Downy', 'Dr', 'Dr.', 'Dreams', 'Drew', 'Drive', 'Drop', 'Dude', 'Dustin', 'Dying', 'ELSE', 'ENOUGH', 'EST', 'EVEN', 'EVERYTHING', 'Earth', 'Easily', 'Eastern', 'Eddie', 'Edgewood', 'Eggs', 'Elev', 'Elle', 'End', 'Eticket', 'Evanescence', 'Even', 'Everyone', 'Everytime', 'Evil', 'Eyes', 'FACE', 'FEMALE', 'FF', 'FINE', 'FL', 'FOLKS', 'FROM']\n"
     ]
    }
   ],
   "source": [
    "# Tokens that start with a capital letter, and contain only alphabetic characters\n",
    "alphabetic = [w for w in chat_words if re.search('^[A-Z][^0-9]+', w)]\n",
    "print(alphabetic[200:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OOooOO :)', 'funny :)', 'haha ;)', 'ty :)', 'hugsss :)', 'ty :)']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sly posts (one word with a smiley face)\n",
    "[p for p in chat_posts if re.search('^[a-zA-z]+\\s(\\;\\)|(\\:\\)))', p)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Daniel <3',\n",
       " 'Marlaya <333333333 !!!!!',\n",
       " 'U190 loves U3 =] <3333',\n",
       " 'heya tiff <3333',\n",
       " \". ACTION <3's all over U197 ..\",\n",
       " '. ACTION is <3 all ovr .']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Posts with <3\n",
    "[p for p in chat_posts if re.search('[a-zA-z]+\\s(\\<3)', p)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1200', '1299', '1900', '1930', '1980', '1985', '1996', '2006']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 digit numbers (Years)\n",
    "[w for w in chat_words if re.search('^[0-9]{4}$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['peace-and-quiet']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double hyphenated phrases\n",
    "# First phrase at least 5 characters long, \n",
    "# second between 2-3 characters\n",
    "# 3rd no more than 6 letters\n",
    "[w for w in chat_words if re.search('^[a-z]{5,}-[a-z]{2,3}-[a-z]{,6}$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['U100', 'U101', 'U102', 'U103', 'U104', 'U105', 'U106', 'U107', 'U108', 'U109', 'U110', 'U111', 'U112', 'U113', 'U114', 'U115', 'U116', 'U117', 'U118', 'U119', 'U120', 'U121', 'U122', 'U123', 'U126', 'U128', 'U129', 'U130', 'U132', 'U133', 'U134', 'U136', 'U137', 'U1370', 'U138', 'U139', 'U141', 'U142', 'U143', 'U144', 'U145', 'U146', 'U147', 'U148', 'U149', 'U150', 'U153', 'U154', 'U155', 'U156', 'U158', 'U163', 'U164', 'U165', 'U168', 'U169', 'U170', 'U172', 'U175', 'U181', 'U190', 'U196', 'U197', 'U219', 'U520', 'U542', 'U819', 'U820', 'U988', 'U989']\n"
     ]
    }
   ],
   "source": [
    "# Capital letter between A-Z, plus 3 numbers between 0-9 (placeholder usernames)\n",
    "print([w for w in chat_words if re.search('^[A-Z][0-9]{3,5}$', w)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Regular Expressions\n",
    "\n",
    "Regular Expressions have a multitude of uses beyond basic search function. The ```re.findall()``` method finds all (non-overlapping) matches of the given regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u', 'e', 'a', 'i', 'a', 'i', 'i', 'i', 'e', 'i', 'a', 'i', 'o', 'i', 'o', 'u']\n"
     ]
    }
   ],
   "source": [
    "# Find all individual vowels\n",
    "word = 'supercalifragilisticexpialidocious'\n",
    "print(re.findall(r'[aeiou]', word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful application is the ability to alter string by removing parts you don't want, such as vowels. We use re.findall() to extract all the matching pieces, and ''.join() to join them together.\n",
    "\n",
    "It's widely noted that English can be comprehended if you remove some of the letters due to the high level of redundancy in it's structures. The regular expression in our next example matches *initial vowel sequences, final vowel sequences, and all consonants; everything else is ignored.* \n",
    "- This three-way disjunction is processed left-to-right, if one of the three parts matches the word, any later parts of the regular expression are ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fnl arc of Slr Mn . She is wll knwn amngst the Glxy fr wrkng hvc and\n",
      "rnng wrlds in hr qst to obtn the strngst Snshi Crystl . She is the mst\n",
      "pwrfl Slr Snshi in the glxy and the rlr of Shdw Glctca ... '' / > Slr\n",
      "Glxia | Slr Mn Wki | FNDM pwrd by Wkia a : lng ( ar ) , a : lng ( ckb\n",
      ") , a\n"
     ]
    }
   ],
   "source": [
    "regexp = r'^[AEIOUaeiou]+|[AEIOUaeiou]+$|[^AEIOUaeiou]'\n",
    "\n",
    "def compress(word):\n",
    "    pieces = re.findall(regexp, word)\n",
    "    return ''.join(pieces)\n",
    "\n",
    "print(nltk.tokenwrap(compress(w) for w in tokens[10:85]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can further develop patterns to get all sequences of two or more vowels (or other such interesting subsets of characters or words), and determine their relative frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('io', 556),\n",
       " ('ea', 494),\n",
       " ('ou', 334),\n",
       " ('ie', 333),\n",
       " ('ai', 269),\n",
       " ('ia', 263),\n",
       " ('ee', 219),\n",
       " ('oo', 176),\n",
       " ('au', 120),\n",
       " ('ua', 110),\n",
       " ('ue', 106),\n",
       " ('ui', 97)]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsj = sorted(set(nltk.corpus.treebank.words()))\n",
    "\n",
    "# Freq Dist of two or more vowels\n",
    "fd = nltk.FreqDist([vs for word in wsj\n",
    "           for vs in re.findall(r'[aeiou]{2,}', word.lower())])\n",
    "\n",
    "fd.most_common(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>e</th>\n",
       "      <th>i</th>\n",
       "      <th>o</th>\n",
       "      <th>u</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>3</td>\n",
       "      <td>504</td>\n",
       "      <td>266</td>\n",
       "      <td>62</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>13</td>\n",
       "      <td>223</td>\n",
       "      <td>336</td>\n",
       "      <td>16</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>272</td>\n",
       "      <td>89</td>\n",
       "      <td>2</td>\n",
       "      <td>66</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o</th>\n",
       "      <td>6</td>\n",
       "      <td>46</td>\n",
       "      <td>586</td>\n",
       "      <td>177</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u</th>\n",
       "      <td>120</td>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "      <td>340</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     a    e    i    o    u\n",
       "a    3  504  266   62  110\n",
       "e   13  223  336   16  111\n",
       "i  272   89    2   66  101\n",
       "o    6   46  586  177   13\n",
       "u  120   23   14  340    1"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Frequency matrix of lower case vowel pairs\n",
    "fd_dist = [vs for word in wsj\n",
    "           for vs in re.findall(r'[aeiou][aeiou]', word.lower())]\n",
    "\n",
    "fd = nltk.ConditionalFreqDist(fd_dist)\n",
    "fd = pd.DataFrame(fd).fillna(value=0).astype(dtype=int)\n",
    "fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "      <th>f</th>\n",
       "      <th>g</th>\n",
       "      <th>h</th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>...</th>\n",
       "      <th>q</th>\n",
       "      <th>r</th>\n",
       "      <th>s</th>\n",
       "      <th>t</th>\n",
       "      <th>u</th>\n",
       "      <th>v</th>\n",
       "      <th>w</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>1</td>\n",
       "      <td>183</td>\n",
       "      <td>288</td>\n",
       "      <td>102</td>\n",
       "      <td>163</td>\n",
       "      <td>135</td>\n",
       "      <td>106</td>\n",
       "      <td>157</td>\n",
       "      <td>100</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>324</td>\n",
       "      <td>145</td>\n",
       "      <td>239</td>\n",
       "      <td>45</td>\n",
       "      <td>74</td>\n",
       "      <td>124</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>110</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>214</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>157</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>142</td>\n",
       "      <td>35</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>188</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>594</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>97</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>7</td>\n",
       "      <td>183</td>\n",
       "      <td>245</td>\n",
       "      <td>477</td>\n",
       "      <td>93</td>\n",
       "      <td>121</td>\n",
       "      <td>197</td>\n",
       "      <td>187</td>\n",
       "      <td>144</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>773</td>\n",
       "      <td>415</td>\n",
       "      <td>630</td>\n",
       "      <td>51</td>\n",
       "      <td>276</td>\n",
       "      <td>95</td>\n",
       "      <td>13</td>\n",
       "      <td>78</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g</th>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>263</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>190</td>\n",
       "      <td>209</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>117</td>\n",
       "      <td>82</td>\n",
       "      <td>176</td>\n",
       "      <td>304</td>\n",
       "      <td>30</td>\n",
       "      <td>171</td>\n",
       "      <td>90</td>\n",
       "      <td>135</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>329</td>\n",
       "      <td>272</td>\n",
       "      <td>597</td>\n",
       "      <td>38</td>\n",
       "      <td>157</td>\n",
       "      <td>83</td>\n",
       "      <td>10</td>\n",
       "      <td>34</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>j</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k</th>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l</th>\n",
       "      <td>341</td>\n",
       "      <td>116</td>\n",
       "      <td>124</td>\n",
       "      <td>9</td>\n",
       "      <td>156</td>\n",
       "      <td>58</td>\n",
       "      <td>28</td>\n",
       "      <td>9</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>38</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m</th>\n",
       "      <td>102</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>127</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n</th>\n",
       "      <td>445</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>492</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>8</td>\n",
       "      <td>1067</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>265</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o</th>\n",
       "      <td>7</td>\n",
       "      <td>124</td>\n",
       "      <td>603</td>\n",
       "      <td>119</td>\n",
       "      <td>10</td>\n",
       "      <td>129</td>\n",
       "      <td>65</td>\n",
       "      <td>138</td>\n",
       "      <td>284</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>149</td>\n",
       "      <td>185</td>\n",
       "      <td>3</td>\n",
       "      <td>51</td>\n",
       "      <td>67</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p</th>\n",
       "      <td>107</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>165</td>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r</th>\n",
       "      <td>395</td>\n",
       "      <td>118</td>\n",
       "      <td>137</td>\n",
       "      <td>74</td>\n",
       "      <td>656</td>\n",
       "      <td>73</td>\n",
       "      <td>108</td>\n",
       "      <td>5</td>\n",
       "      <td>83</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>109</td>\n",
       "      <td>6</td>\n",
       "      <td>252</td>\n",
       "      <td>153</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s</th>\n",
       "      <td>178</td>\n",
       "      <td>28</td>\n",
       "      <td>18</td>\n",
       "      <td>51</td>\n",
       "      <td>492</td>\n",
       "      <td>7</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>221</td>\n",
       "      <td>146</td>\n",
       "      <td>158</td>\n",
       "      <td>137</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t</th>\n",
       "      <td>432</td>\n",
       "      <td>5</td>\n",
       "      <td>148</td>\n",
       "      <td>3</td>\n",
       "      <td>141</td>\n",
       "      <td>25</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>651</td>\n",
       "      <td>86</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u</th>\n",
       "      <td>69</td>\n",
       "      <td>104</td>\n",
       "      <td>94</td>\n",
       "      <td>69</td>\n",
       "      <td>15</td>\n",
       "      <td>79</td>\n",
       "      <td>64</td>\n",
       "      <td>38</td>\n",
       "      <td>10</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>73</td>\n",
       "      <td>70</td>\n",
       "      <td>208</td>\n",
       "      <td>84</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v</th>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>33</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>52</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>15</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>27</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     a    b    c    d    e    f    g    h     i   j ...   q    r    s    t  \\\n",
       "a    1  183  288  102  163  135  106  157   100  23 ...   0  324  145  239   \n",
       "b  110   14    1    1   12    1    0    5    35   0 ...   0   27    7    3   \n",
       "c  214    4   17    4  157    1    0    1   256   0 ...   0   70  142   35   \n",
       "d  188    1    3   20  594    0    2    1    98   0 ...   0   97    8    1   \n",
       "e    7  183  245  477   93  121  197  187   144  25 ...   0  773  415  630   \n",
       "f   36    0    0    3   29   36    1    3    53   0 ...   0   15   13    0   \n",
       "g  110    0    0   25   20    0   19    0    68   0 ...   0   64    1    0   \n",
       "h    5    2  263    1    7    0   94    2     1   0 ...   0    9  190  209   \n",
       "i  117   82  176  304   30  171   90  135     4   6 ...   0  329  272  597   \n",
       "j    0    3    0    0    1    0    1    0     1   0 ...   0    1    0    0   \n",
       "k   36    0  114    0   15    1    1    1     4   0 ...   0   63   34    0   \n",
       "l  341  116  124    9  156   58   28    9   168   0 ...   0   53   53   38   \n",
       "m  102    5    3    4  127    0    8    9   115   0 ...   0   98   37    5   \n",
       "n  445    0    0   10  492    0   57    8  1067   0 ...   0   93    7    7   \n",
       "o    7  124  603  119   10  129   65  138   284  44 ...   0  165  149  185   \n",
       "p  107    1    0    1   31    0    0    0    69   0 ...   0   43  165    2   \n",
       "q    2    0    0    0   28    0    0    1     2   0 ...   0    0    5    0   \n",
       "r  395  118  137   74  656   73  108    5    83   2 ...   0  109    6  252   \n",
       "s  178   28   18   51  492    7   35   10   172   0 ...   0  221  146  158   \n",
       "t  432    5  148    3  141   25    9   29   264   0 ...   0  200  651   86   \n",
       "u   69  104   94   69   15   79   64   38    10  53 ...  73   70  208   84   \n",
       "v   33    0    0    2   68    0    0    0   105   0 ...   0   52    3    1   \n",
       "w   27    0    0    7   35    0    0    2     4   0 ...   0   17   33   25   \n",
       "x    3    0    0    0  150    0    0    0     2   0 ...   0    0    0    1   \n",
       "y   52   13   23   15   32    4   13   13     0   0 ...   0   73   27   76   \n",
       "z    5    0    2    0    2    1    0    0    47   0 ...   0    0    0    2   \n",
       "\n",
       "     u    v    w   x   y   z  \n",
       "a   45   74  124   7  19  15  \n",
       "b   17    0    0   0   8   0  \n",
       "c   57    0    2   0   5   0  \n",
       "d   37    0    2   0   1   1  \n",
       "e   51  276   95  13  78  37  \n",
       "f    2    0    1   0   1   0  \n",
       "g   37    0    1   0   0   0  \n",
       "h    0    0   51   0   0   2  \n",
       "i   38  157   83  10  34  13  \n",
       "j    0    1    0   0   0   0  \n",
       "k    2    0    1   0   0   0  \n",
       "l   77    0    5   0  12   0  \n",
       "m   32    0    4   0  11   0  \n",
       "n  265    0   27   0   4   1  \n",
       "o    3   51   67   4  32   7  \n",
       "p   44    0    1   2   4   0  \n",
       "q    0    0    0   0   1   0  \n",
       "r  153    0   22   0   4   0  \n",
       "s  137    1   24   0  32   0  \n",
       "t  108    0    2  12   9   0  \n",
       "u    1    0    1   2   0   2  \n",
       "v    1    1    0   0   0   0  \n",
       "w    0    0    1   1   5   0  \n",
       "x    3    0    0   0   0   0  \n",
       "y    0    2    5   1   0   4  \n",
       "z    2    0    0   0   1   5  \n",
       "\n",
       "[26 rows x 26 columns]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Frequency matrix of lower case letter pairs\n",
    "cfd_nums = [vs for word in wsj\n",
    "            for vs in re.findall(r'[a-z][a-z]', word.lower())]\n",
    "\n",
    "cfd = nltk.ConditionalFreqDist(cfd_nums)\n",
    "df_cfd = pd.DataFrame(cfd).fillna(value=0).astype(dtype=int)\n",
    "df_cfd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make use of these word segments, it would be really useful to be able to see the actual words that these frequencies come from in the event that we have questions about patterns that arise in the data. We can look for patterns such as *partial complimentary distributions*, which suggest things like letter pairs not being a distinct **phoneme** (perceptually distinct units of sound used to distinguish one word from another) in the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfd_pairs = [(cv, w) for w in wsj\n",
    "            for cv in re.findall(r'[a-z][a-z]', w.lower())]\n",
    "\n",
    "cfd_index = nltk.Index(cfd_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bowman', 'Lawmakers', 'lawmakers', 'lawmaking']\n",
      "['Fahrenheit', 'Mehrens', 'Schroder', 'cutthroat', 'synchronized']\n",
      "['defying', 'identify', 'modify', 'notify']\n"
     ]
    }
   ],
   "source": [
    "print(cfd_index['wm'])\n",
    "print(cfd_index['hr'])\n",
    "print(cfd_index['fy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aq': ['Aquino', 'Nasdaq'], 'aa': ['Jalaalwalikraam'], 'qa': []}\n"
     ]
    }
   ],
   "source": [
    "# Find Indexes with [AQ] letter pairs\n",
    "aq = [w for w in cfd_index if re.search('^[aq]+$', w)]\n",
    "print({k:cfd_index[k] for k in aq if k in cfd_index})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'xy': ['sexy']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find Indexes with [FY] letter pairs\n",
    "yx = [w for w in cfd_index if re.search('^[yx]+$', w)]\n",
    "display({k:cfd_index[k] for k in yx if k in cfd_index})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Word Stems\n",
    "\n",
    "A lot of word processing happens around us that we hardly even notice. For example, when we search on the web, results are concatenated by similar terms, and we usually pay it know mind. In fact, it's a behavior we've come to expect for a good user experience. *Computer* and *Computers* are just two forms of the same dictionary word, or *lemma*. For many processing tasks, we need to be able to ignore word endings, and deal directly with word stems.\n",
    "\n",
    "There are many ways of doing this, some more cleanly than others, but Regexes offer yet another means of doing so quickly and efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "suffixes = ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to make a regex that will take these suffixes and chop them off words. In this example, we'll pull a random word out of our index using the letter pairs we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ment']"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^.*(ing|ly|ed|ious|ies|ive|es|s|ment)$', cfd_index['rt'][10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```re.findall()``` just gave us the suffix even though the regular expression matched the entire word. This is because the parentheses have a second function, to select substrings to be extracted. If we want to use the parentheses to specify the scope of the disjunction, but not to select the material to be output, we have to add ```?:```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Department']"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^.*(?:ing|ly|ed|ious|ies|ive|es|s|ment)$', cfd_index['rt'][10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is good progress since now we have the word stem instead of the suffix, but instead of just returning one or the other, we should make the Regex give us both parts back separately. This can be done by parenthesising the arguments in the regex pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Depart', 'ment')]"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', cfd_index['rt'][10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is looking like what we would expect of a stemmer, but what happens if we try some more word stems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Preference', 's')]"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', cfd_index['pr'][13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of 'Preferences', the word stem should have been '-es'. This happened because the ```\\*``` operator is greedy, so ```.\\*``` tries to eat as much of the input as possible. If we change the expression to use the non-greedy version of the \\* operator, ```*?```, it should instead feed that e into the other output where it's expected. We can even add the same to parentheses for the suffix, to make that content all together optional, thus allowing empty suffixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Preferenc', 'es')]"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$', cfd_index['pr'][13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Depart', 'ment')]\n",
      "['Participant', 'Particular', 'Part', 'Partner', 'Partnership', 'Party', 'Porter', 'Port', 'Portugal', 'Report', 'Robert', 'Robertson', 'Stuart', 'Uncertainty', 'Unfortunate', 'Virtual', 'Wadsworth', 'Wertheim', 'Westport', 'Woolworth']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)$', cfd_index['rt'][10]))\n",
    "def stem(word):\n",
    "    regexp = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\n",
    "    stem, suffix = re.findall(regexp, word)[0]\n",
    "    return stem\n",
    "\n",
    "stems = [stem(t) for t in cfd_index['rt'][40:60]]\n",
    "\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's still a lot of problems here, but overall this is a good general understanding of how stemming works, and how it can be done smarter by using Regexes. At the end of the day, you're almost always going to want to use a tried and true readily available stemmer such as the one already built into NLTK. In general, it's important to understand the fundamentals of how the process of stemming works, and what the concept of lemmatization is. \n",
    "\n",
    "### Searching Tokenized Text\n",
    "\n",
    "Regexes can also be formatted in a manner tha allows you to search across multiple words in a corpus. For example, if we were curious about the impact of a particular event, we could search for patterns involving <the> <event> and see what comes up in context. Angle brackets are used to mark token boundaries, and any whitespace between the angle brackets is ignored **(behaviors that are unique to NLTK's ```findall()``` method for texts)**\n",
    "\n",
    "It is easy to build search patterns when the linguistic phenomenon we're studying is tied to particular words. In some cases, a little creativity will go a long way. For instance, searching a large text corpus for expressions of the form ```x and other ys``` allows us to discover **hypernyms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speed and other activities; water and other liquids; tomb and other\n",
      "landmarks; Statues and other monuments; pearls and other jewels;\n",
      "charts and other items; roads and other features; figures and other\n",
      "objects; military and other areas; demands and other factors;\n",
      "abstracts and other compilations; iron and other metals\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "hobbies_learned = nltk.Text(brown.words(categories=['hobbies', 'learned']))\n",
    "hobbies_learned.findall(r\"<\\w*> <and> <other> <\\w*s>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with metal or other hard material; the rain or other inclement\n",
      "weather; that Jones or other Confederate commanders; national forest\n",
      "or other lands under; a tree or other object appears; on acetate or\n",
      "other types of; other changes or other energies involved; of State or\n",
      "other public officials; the president or other executive of; the\n",
      "community or other school district; No soap or other cleaning agent; a\n",
      "thermometer or other equivalent equipment\n"
     ]
    }
   ],
   "source": [
    "hobbies_learned.findall(r\"<\\w*> <\\w*> <or> <other> <\\w*> <\\w*>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "campaigners and not only; Delawares and not much; official and not\n",
      "just; sided and not balanced; 20 and not more; you and not cutting;\n",
      "chlorine and not by; dispersed and not politically; lawmaking and not\n",
      "leave; aside and not used; district and not that; expenses and not\n",
      "the; moral and not by; hypothesis and not a; books and not out; will\n",
      "and not inclined\n"
     ]
    }
   ],
   "source": [
    "hobbies_learned.findall(r\"<\\w*> <and> <not> <\\w*>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With enough text, this approach would give us a useful store of information about the taxonomy of objects, without the need for any manual labor**\n",
    "- However, our search results will usually contain false positives\n",
    "  -  For example, the result: ```demands and other factors``` suggests that demand is an instance of the type factor, but this sentence is actually about wage demands\n",
    "  -  On the inverse, these searches will also suffer from false negatives, where cases are omitted that you would want to be included.\n",
    "-  Nevertheless, we could construct our own ontology of English concepts by manually correcting the output of such searches. \n",
    "  -  Generally, there is a balance stuck in some combination of automatic and manual processing.\n",
    "  \n",
    "## Normalizing Text\n",
    "\n",
    "As we've processed texted thus far, one of the steps along the way has been to call the ```.lower()``` function. By calling this function, we've normalized the text by getting rid of upper case characters, and ignoring distinctions between upper and lower casing in type face. We then went a step further into stemming with Regexes.\n",
    "\n",
    "### Lemmatization\n",
    "\n",
    "Regexes...w hile quick and dirty, and good for certain applications, are not good for others. Other applications will demand proper dictionary words be returned, and not partial words such as 'Preferenc'. This is where the task known as lemmatization comes back in. We'll cover this by going over a few of NLTKs built in stemmers. **Stemming is not a well-defined process, and we typically pick the stemmer that best suits the application we have in mind**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = chat_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IndexedText(object):\n",
    "\n",
    "    def __init__(self, stemmer, text):\n",
    "        self._text = text\n",
    "        self._stemmer = stemmer\n",
    "        self._index = nltk.Index((self._stem(word), i)\n",
    "                                 for (i, word) in enumerate(text))\n",
    "\n",
    "    def concordance(self, word, width=40):\n",
    "        key = self._stem(word)\n",
    "        wc = int(width/4)                # words of context\n",
    "        for i in self._index[key]:\n",
    "            lcontext = ' '.join(self._text[i-wc:i])\n",
    "            rcontext = ' '.join(self._text[i:i+wc])\n",
    "            ldisplay = '{:>{width}}'.format(lcontext[-width:], width=width)\n",
    "            rdisplay = '{:{width}}'.format(rcontext[:width], width=width)\n",
    "            print(ldisplay, rdisplay)\n",
    "\n",
    "    def _stem(self, word):\n",
    "        return self._stemmer.stem(word).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ```PorterStemmer()``` is a good choice if you are indexing some texts and want to support search using alternative forms of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aww', 'awww', 'B', 'baaaaalllllllliiiiiiinnnnnnnnnnn', 'BE', 'big', 'blond', 'boot', 'booti', 'boy', 'but', 'but', 'bye', 'back', 'barbiee', 'baromet', 'beach', 'becaus', 'been', 'ben', 'benjamin', 'better', 'bibl', 'biiiiiitch', 'biographi', 'birdgang', 'bloooooooood', 'bloooooooooood', 'bloooooooooooood', 'bone', 'bonu', 'book', 'boon', 'booyah', 'borat', 'born', 'box', 'boyz', 'break', 'break', 'broken', 'bud', 'burger', 'but', 'bwhaha', 'bye', 'C', 'CA', 'cali', 'can']\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "print([porter.stem(t) for t in tokens[500:550]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stem_porter = IndexedText(porter, chat_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not tried myspace for dating . JOIN JOIN damn U15 .. mopeds r for old men and gay\n",
      "ts me all tingly n stuff bye honey bunny damn 19 / F / Wisconsin ooeer is sum1 go\n",
      "ie like U12 or come haha did , no takers damn knows what everyone gets U44 for x-\n",
      " is having issues . ACTION waves to U5 . damn PART JOIN so U88 - why no pic . ACT\n",
      "fy PART JOIN you never stay and chat U69 damn lol aww looks for D :beer: chatting\n",
      "ve JOIN hahahahahahahahahahahahahahahaha damn hey ppl u twizted bro hey guys MY D\n",
      ". pm me if you want to chat .... yea wow damn Barbieee . PART 15 f cali guys pm m\n",
      "is fun haha PART i am satan and u r dead damn not me this room is bluer than my b\n",
      "pewing goodie pm me haha hurling yes lol damn . ACTION * Pearl Jam - Better Man .\n",
      "TION chops U34 's finers AND toes off .. DAMN JOIN . ACTION Fingers * . What did \n"
     ]
    }
   ],
   "source": [
    "stem_porter.concordance('damn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cap', 'cdt', 'chat', 'chathid', 'chip', 'choco', 'co', 'com', 'com', 'csi', 'cst', 'ct', 'cuz', 'californ', 'cam', 'can', 'canehd', 'cardin', 'cardn', 'card', 'car', 'carolin', 'catterick', 'ceil', 'chamillionair', 'chang', 'chang', 'chat', 'check', 'check', 'cheeeez', 'chic', 'chick', 'childr', 'chin', 'chingy', 'chop', 'chris', 'christianity', 'ciar', 'city', 'cleveland', 'clock', 'coincid', 'com', 'comply', 'connect', 'connecticut', 'consid', 'constitut']\n"
     ]
    }
   ],
   "source": [
    "lancaster = nltk.LancasterStemmer()\n",
    "print([lancaster.stem(t) for t in tokens[550:600]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The ```WordNetLemmatizer()``` only removes affixes if the resulting word is in its dictionary\n",
    "* The ```WordNetLemmatizer()``` is a good choice if you want to compile the vocabulary of some texts and want a list of valid lemmas (or lexicon headwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cookies', 'Cool', 'Could', 'Course', 'Covered', 'Cradle', 'Craig', 'Crazy', 'Cream', 'Cry', 'Ct', 'Ctrl', 'Cum', 'Current', 'Cute', 'Cyber', 'D', 'DAMN', 'DAamn', 'DELIGHTFUL', 'DETROIT', 'DING', 'DIRTY', 'DJ', 'DO', 'DOES', 'DOING', 'DON', 'DONT', 'DOWNS', 'DVD', 'Dakota', 'Damn', 'Dang', 'Daniel', 'Daveeee', 'David', 'Dawn', 'Dawnstar', 'Days', 'Death', 'Deep', 'Define', 'Denver', 'Depends', 'Devil', 'Dew', 'Diary', 'Did', 'Diego']\n"
     ]
    }
   ],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "print([wnl.lemmatize(t)  for t in tokens[600:650]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another normalization task involves identifying non-standard words including numbers, abbreviations, and dates, and mapping any such tokens to a special vocabulary. \n",
    "-  For example, every decimal number could be mapped to a single token 0.0, and every acronym could be mapped to AAA. \n",
    "-  This keeps the vocabulary small and improves the accuracy of many language modeling tasks\n",
    "\n",
    "## Tokenizing Text with Regexes\n",
    "\n",
    "Tokenization is the task of cutting a string into identifiable linguistic units that constitute a piece of language data. The very simplest method for tokenizing text is to split on whitespace.\n",
    "\n",
    " - When using a regular expression, you must also account for tabs and new lines\n",
    " - This can be further refined into splitting the text on anything other than a word character\n",
    " - We can use \\W in a simple regular expression to split the input on anything other than a word character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['up', 'and', 'picking', 'the', 'daisies', 'when', 'suddenly', 'a', 'White', 'Rabbit', 'with', 'pink', 'eyes', 'ran', 'close', 'by', 'her', 'There', 'was', 'nothing', 'so', 'very', 'remarkable', 'in', 'that', 'nor', 'did', 'Alice', 'think', 'it', 'so', 'very', 'much', 'out', 'of', 'the', 'way', 'to', 'hear', 'the', 'Rabbit', 'say', 'to', 'itself', 'Oh', 'dear', 'Oh', 'dear', 'I', 'shall']\n"
     ]
    }
   ],
   "source": [
    "re_split_raw = re.split(r'[ \\W]+', raw)\n",
    "print(re_split_raw[100:150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Symbol | Function | \n",
    "|-----|-----|-----|\n",
    "| \\b | Word boundary (zero width) | \n",
    "| \\d | Any decimal digit (equivalent to [0-9]) | \n",
    "| \\D | Any non-digit character (equivalent to [^0-9]) | \n",
    "| \\s | Any whitespace character (equivalent to [ \\t\\n\\r\\f\\v]) | \n",
    "| \\S | Any non-whitespace character (equivalent to [^ \\t\\n\\r\\f\\v]) | \n",
    "| \\w | Any alphanumeric character (equivalent to [a-zA-Z0-9_]) | \n",
    "| \\W | Any non-alphanumeric character (equivalent to [^a-zA-Z0-9_]) | \n",
    "| \\t | The tab character | \n",
    "| \\n | The newline character | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You will see empty strings at the start and the end with certain texts, to demonstrate why, we split a string with ```xx``` on ```x```. We get the same tokens, but without the empty strings, with  ```re.findall(r'\\w+', raw)```, using a pattern that matches the words instead of the spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '']"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'xx'.split('x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['up', 'and', 'picking', 'the', 'daisies', 'when', 'suddenly', 'a', 'White', 'Rabbit', 'with', 'pink', 'eyes', 'ran', 'close', 'by', 'her', 'There', 'was', 'nothing', 'so', 'very', 'remarkable', 'in', 'that', 'nor', 'did', 'Alice', 'think', 'it', 'so', 'very', 'much', 'out', 'of', 'the', 'way', 'to', 'hear', 'the', 'Rabbit', 'say', 'to', 'itself', 'Oh', 'dear', 'Oh', 'dear', 'I', 'shall']\n"
     ]
    }
   ],
   "source": [
    "re_split_raw = re.findall(r'\\w+', raw)\n",
    "print(re_split_raw[100:150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that words are being matched the way we expect, we can delve into extending the functionality. Lets say we want to match any sequence of word characters... if no match is found, we want to try to match any non-whitespace character (\\S is the compliment to \\s) followed by other word characters.\n",
    "\n",
    "This means that punctuation is grouped with any following letters (e.g. 's) but that sequences of two or more punctuation characters are separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pleasure', 'of', 'making', 'a', 'daisy', '-chain', 'would', 'be', 'worth', 'the', 'trouble', 'of', 'getting', 'up', 'and', 'picking', 'the', 'daisies', ',', 'when', 'suddenly', 'a', 'White', 'Rabbit', 'with', 'pink', 'eyes', 'ran', 'close', 'by', 'her', '.', 'There', 'was', 'nothing', 'so', 'very', 'remarkable', 'in', 'that', ',', 'nor', 'did', 'Alice', 'think', 'it', 'so', 'very', 'much', 'out']\n"
     ]
    }
   ],
   "source": [
    "re_split_raw = re.findall(r'\\w+|\\S\\w*', raw)\n",
    "print(re_split_raw[100:150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could further generalize to allow words with hyphens, ellipisism, open parenthesises, and apostrophes inside the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pleasure', 'of', 'making', 'a', 'daisy-chain', 'would', 'be', 'worth', 'the', 'trouble', 'of', 'getting', 'up', 'and', 'picking', 'the', 'daisies', ',', 'when', 'suddenly', 'a', 'White', 'Rabbit', 'with', 'pink', 'eyes', 'ran', 'close', 'by', 'her', '.', 'There', 'was', 'nothing', 'so', 'very', 'remarkable', 'in', 'that', ',', 'nor', 'did', 'Alice', 'think', 'it', 'so', 'very', 'much', 'out', 'of']\n"
     ]
    }
   ],
   "source": [
    "re_split_raw = re.findall(r\"\\w+(?:[-']\\w+)*|'|[-.(]+|\\S\\w*\", raw)\n",
    "print(re_split_raw[100:150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Tokenization turns out to be a far more difficult task than you might have expected. No single solution works well across-the-board, and we must decide what counts as a token depending on the application domain.\n",
    "\n",
    "When developing a tokenizer it helps to have access to raw text which has been manually tokenized, in order to compare the output of your tokenizer with high-quality (or \"gold-standard\") tokens.\n",
    "\n",
    "A final issue for tokenization is the presence of contractions, such as didn't. If we are analyzing the meaning of a sentence, it would probably be more useful to normalize this form to two separate forms: did and n't (or not). We can do this work with the help of a lookup table.\n",
    "\n",
    "### NLTK Regex Tokenizer\n",
    "\n",
    "The function ```nltk.regexp_tokenize()``` is similar to ```re.findall()``` (as we've been using it for tokenization). However, ```nltk.regexp_tokenize()``` is more efficient for this task, and avoids the need for special treatment of parentheses.\n",
    "\n",
    "We can evaluate a tokenizer by comparing the resulting tokens with a wordlist, and reporting any tokens that don't appear in the wordlist, using ```set(tokens).difference(wordlist)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pleasure', 'of', 'making', 'a', 'daisy-chain', 'would', 'be', 'worth', 'the', 'trouble', 'of', 'getting', 'up', 'and', 'picking', 'the', 'daisies', ',', 'when', 'suddenly', 'a', 'White', 'Rabbit', 'with', 'pink', 'eyes', 'ran', 'close', 'by', 'her', '.', 'There', 'was', 'nothing', 'so', 'very', 'remarkable', 'in', 'that', ',', 'nor', 'did', 'Alice', 'think', 'it', 'so', 'very', 'much', 'out', 'of']\n"
     ]
    }
   ],
   "source": [
    "re_tokenize_raw = nltk.regexp_tokenize(raw, r\"\\w+(?:[-']\\w+)*|'|[-.(]+|\\S\\w*\")\n",
    "\n",
    "print(re_tokenize_raw[100:150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation\n",
    "\n",
    "Tokenization is a more specific practice of the more general concept of segmentation. Segmentation thus far has been done in a very specific manner, however there are many other ways in which a text corpus can be segmented in which to gain knowledge from it.\n",
    "\n",
    "### Sentence Segementation\n",
    "\n",
    "Manipulating texts at the level of individual words often presupposes the ability to divide a text into individual sentences.. Some corpora, such as the Brown corpus, already provide access to the data at the sentence level. In other cases, the text will need to be manually processed into sentences first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Words Per Snt Brown Corpus:  20.250994070456922\n"
     ]
    }
   ],
   "source": [
    "print('Avg Words Per Snt Brown Corpus: '\\\n",
    "      , len(nltk.corpus.brown.words()) / len(nltk.corpus.brown.sents()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"I will, then,\" said the little Red Hen, and she planted the grain of\\r\\n'\n",
      " 'wheat.',\n",
      " 'When the wheat was ripe she said, \"Who will take this wheat to the\\r\\nmill?\"',\n",
      " '\"Not I,\" said the Goose.',\n",
      " '\"Not I,\" said the Duck.',\n",
      " '\"I will, then,\" said the little Red Hen, and she took the wheat to the\\r\\n'\n",
      " 'mill.',\n",
      " 'When she brought the flour home she said, \"Who will make some bread with\\r\\n'\n",
      " 'this flour?\"',\n",
      " '\"Not I,\" said the Goose.',\n",
      " '\"Not I,\" said the Duck.',\n",
      " '\"I will, then,\" said the little Red Hen.',\n",
      " 'When the bread was baked, she said, \"Who will eat this bread?\"']\n"
     ]
    }
   ],
   "source": [
    "text = nltk.corpus.gutenberg.raw('bryant-stories.txt')\n",
    "sents = nltk.sent_tokenize(text)\n",
    "pp.pprint(sents[79:89])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence segmentation is difficult because period is used to mark abbreviations, and some periods simultaneously mark an abbreviation and terminate a sentence, as often happens with acronyms like U.S.A.\n",
    "\n",
    "### Word Segmentation\n",
    "\n",
    "For some writing systems, tokenizing text is made more difficult by the fact that there is no visual representation of word boundaries. For example, in Chinese, the three-character string: 爱国人 (ai4 \"love\" (verb), guo2 \"country\", ren2 \"person\") could be tokenized as 爱国 / 人, \"country-loving person\" or as 爱 / 国人, \"love country-person.\"\n",
    "\n",
    "A similar problem arises in the processing of spoken language, where the hearer must segment a continuous speech stream into individual words. A particularly challenging version of this problem arises when we don't know the words in advance. This is the problem faced by a language learner, such as a child hearing utterances from a parent. Consider the following artificial example, where word boundaries have been removed:\n",
    "\n",
    "1.  doyouseethekitty\n",
    "2.  seethedoggy\n",
    "3.  doyoulikethekitty\n",
    "4.  likethedoggy\n",
    "\n",
    "Our first challenge is simply to represent the problem: we need to find a way to separate text content from the segmentation. We can do this by annotating each character with a boolean value to indicate whether or not a word-break appears after the character (an idea that will be used heavily for \"chunking\").\n",
    "\n",
    "Let's assume that the learner is given the utterance breaks, since these often correspond to extended pauses. Here is a possible representation, including the initial and target segmentations.\n",
    "\n",
    "-  The segmentation strings consist of zeros and ones\n",
    "-  They are one character shorter than the source text\n",
    "  -  A text of length ```n``` can only be broken up in ```n-1``` places.\n",
    "  -  ```seg1``` and ```seg2``` represent the initial and final segmentations of some hypothetical child-directed speech\n",
    "  -  The ```segment()``` function can use them to reproduce the segmented text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unsegemented text string with word and sentence representations within\n",
    "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
    "\n",
    "# Known binary representations of sentence(seg1) and word (seg2) segmentation in text\n",
    "seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
    "seg2 = \"0100100100100001001001000010100100010010000100010010000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Segment text based on location of 1/0 in the segment index\n",
    "def segment(text, segs):\n",
    "    segments = []\n",
    "    last = 0\n",
    "    for i in range(len(segs)):\n",
    "        if segs[i] == '1':                  # Segment whenever i = 1\n",
    "            segments.append(text[last:i+1]) # Append segment length equal to [last:i+1]\n",
    "            last = i+1                      # Increment last to start the next segment\n",
    "    segments.append(text[last:])            # Catch the last segment\n",
    "    return segments                         # Return all identified segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Segment Known Sentences\n",
    "segment(text, seg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['do', 'you', 'see', 'the', 'kitty', 'see', 'the', 'doggy', 'do', 'you', 'like', 'the', 'kitty', 'like', 'the', 'doggy']\n"
     ]
    }
   ],
   "source": [
    "# Test Segment Known Words\n",
    "print(segment(text, seg2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It may have seemed a little abstract at first, but now we can visualize how the segmentation comes together, and the words and sentences are formed using this type of abstraction. Now the segmentation task becomes a search problem: **find the bit string that causes the text string to be correctly segmented into words.**\n",
    "\n",
    "\n",
    "<img src='https://www.nltk.org/images/brent.png'>\n",
    "\n",
    ">-  We assume the learner is acquiring words and storing them in an internal lexicon. \n",
    ">-  Given a suitable lexicon, it is possible to reconstruct the source text as a sequence of lexical items\n",
    ">-  We can define an objective function as a scoring function whose value we optimize \n",
    ">  - This is based on the size of the lexicon and the amount of information needed to reconstruct the source text\n",
    "<br>\n",
    "\n",
    "> **Calculation of Objective Function, Given a hypothetical segmentation of the source text:**\n",
    ">1.  Derive a lexicon and a derivation table that permit the source text to be reconstructed\n",
    ">2.  Total up the number of characters used by each lexical item (including a boundary marker), plus the number of lexical items used by each derivation\n",
    ">  -  This serves as a score of the quality of the segmentation\n",
    ">  -  Smaller values of the score indicate a better segmentation\n",
    ">3.  Sum the total of the derived and lexicon table lengths\n",
    "\n",
    "### Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Objective funtion to evaluate and score segmentation of bit strings into words\n",
    "def evaluate(text, segs):\n",
    "    words = segment(text, segs) # Segment words\n",
    "    text_size = len(words)      # Derived Lexicon Size Table\n",
    "    \n",
    "    # Objective Function Calculation (Lexicon Size)\n",
    "    # =============================================================================\n",
    "    # sum(len(word) + 1 for word in set(words))\n",
    "    #\n",
    "    # len(word)               = num characters used by each lexical item\n",
    "    # + 1                     = a boundary marker\n",
    "    # for word in set(words)  = num of lexical items used by each derivation\n",
    "    #\n",
    "    lexicon_size = sum(len(word) + 1 for word in set(words))\n",
    "    return text_size + lexicon_size                          # Derived Lexicon + Lexicon Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seg3 = \"0000100100000011001000000110000100010000001100010000001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seg4 = \"0001001000000101001011000101001000100010011111100010110\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['doyou', 'see', 'thekitt', 'y', 'see', 'thedogg', 'y', 'doyou', 'like', 'thekitt', 'y', 'like', 'thedogg', 'y']\n"
     ]
    }
   ],
   "source": [
    "print(segment(text, seg3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n",
      "48\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(text, seg3))\n",
    "print(evaluate(text, seg2))\n",
    "print(evaluate(text, seg1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(text, seg4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to **search for the pattern of zeros and ones that minimizes this objective function**. We can accomplish this through a process known as **annealing**. \n",
    "\n",
    ">Per Katrina Ellison Gelman's <a href='http://katrinaeg.com/simulated-annealing.html'>blog on simulated annealing</a>, simulated annealing is 'a method for finding a good (not necessarily perfect) solution to an optimization problem'. She ellaborates, \n",
    "-  'Broadly, an optimization algorithm searches for the best solution by generating a random initial solution and \"exploring\" the area nearby\n",
    "-  If a neighboring solution is better than the current one, then it moves to it\n",
    "-  If not, then the algorithm stays put.'\n",
    "\n",
    "Lastly, Katrina gives a very clear basic outline of the algorithm that is required to minimize our objective function through annealing:\n",
    "\n",
    "1. First, generate a random solution\n",
    "2. Calculate its cost using some cost function you've defined\n",
    "3. Generate a random neighboring solution\n",
    " -  \"Neighboring\" means there's only one thing that differs between the old solution and the new solution. Effectively, you switch two elements of your solution and re-calculate the cost.\n",
    "4. Calculate the new solution's cost\n",
    "5. Compare them:\n",
    "  1. If cnew < cold: move to the new solution\n",
    "     - This is good, smaller is better, so the new one is retained\n",
    "  2. If cnew > cold: maybe move to the new solution\n",
    "     -  Sometimes the lower value is retained as not to get trapped in local maxima\n",
    "     -  This is not implemented in this solution \n",
    "  3. Repeat steps 3-5 above until an acceptable solution is found or you reach some maximum number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "# Flips the bit sign (1/0) of segment bit string at index position\n",
    "def flip(segs, pos):\n",
    "    flip = str(1-int(segs[pos]))              # Inverts 1/0 at specified position\n",
    "    return segs[:pos] + flip + segs[pos+1:]   # Puts inverted value back in string\n",
    "\n",
    "# Flips n randomly selected indexes in bit string\n",
    "def flip_n(segs, n):\n",
    "    for i in range(n):    # Flip random position indexes n times\n",
    "        segs = flip(segs, randint(0, len(segs)-1))\n",
    "    return segs           # Generates new random bit string\n",
    "\n",
    "# 'Heat and Allow to Cool Slowly': A Metaphor for our Optimization Algorithm\n",
    "#\n",
    "# Args: text: text\n",
    "# segs: bit string\n",
    "# iterations: max iterations\n",
    "# cool: rate to cool temperature each iteration\n",
    "#\n",
    "def anneal(text, segs):\n",
    "    guesses = 1000                      # Healthy minimum is 100 - 1000\n",
    "    cool = .95                             # Generally alpha between .8 and .99\n",
    "    T = float(len(segs))                   # Tempurature: f() of current iteration\n",
    "    \n",
    "\n",
    "    while T > 0.1:                         # Usually between 0 and 1             \n",
    "        # best_segs = segs                 # Retain segs as best_segs\n",
    "        # best = evaluate(text, segs)      # Evaluate score of segs\n",
    "        best_segs, best = segs, evaluate(text, segs)\n",
    "        \n",
    "        # Compare random bit strings to current best bit string iteratively\n",
    "        for i in range(guesses):           \n",
    "            guess = flip_n(segs, round(T))    # Generate new random bit string\n",
    "            score = evaluate(text, guess)     # Evaluate score of random bit string\n",
    "         \n",
    "            if score < best:                  # If lower score than best is found:\n",
    "                # best = score                # Retain the new score as best score\n",
    "                # best_segs = guess           # Retain new segment as best segment\n",
    "                best, best_segs = score, guess\n",
    "        \n",
    "        # Prep for next loop or hit max      \n",
    "        score, segs = best, best_segs         # Save score and segs\n",
    "        T = T * cool                          # Cool tempurature for next loop\n",
    "        \n",
    "        print(evaluate(text, segs)            # Display report of evaluation score\n",
    "            , segment(text, segs))            # and segmented words each cooloff\n",
    "    print()\n",
    "    return segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 ['d', 'oyo', 'u', 'se', 'e', 't', 'h', 'e', 'ki', 't', 'tyseet', 'hedoggydoyoulik', 'e', 'thekit', 't', 'ylikethe', 'doggy']\n",
      "78 ['d', 'oyous', 'e', 'e', 't', 'heki', 't', 'ty', 'seethe', 'doggy', 'doy', 'ouliketh', 'e', 'k', 'itt', 'ylikethe', 'doggy']\n",
      "78 ['d', 'oyous', 'e', 'e', 't', 'heki', 't', 'ty', 'seethe', 'doggy', 'doy', 'ouliketh', 'e', 'k', 'itt', 'ylikethe', 'doggy']\n",
      "78 ['d', 'oyous', 'e', 'e', 't', 'heki', 't', 'ty', 'seethe', 'doggy', 'doy', 'ouliketh', 'e', 'k', 'itt', 'ylikethe', 'doggy']\n",
      "78 ['d', 'oyous', 'e', 'e', 't', 'heki', 't', 'ty', 'seethe', 'doggy', 'doy', 'ouliketh', 'e', 'k', 'itt', 'ylikethe', 'doggy']\n",
      "78 ['d', 'oyous', 'e', 'e', 't', 'heki', 't', 'ty', 'seethe', 'doggy', 'doy', 'ouliketh', 'e', 'k', 'itt', 'ylikethe', 'doggy']\n",
      "77 ['do', 'you', 's', 'eethekit', 'tyseeth', 'e', 'do', 'ggydo', 'you', 'like', 't', 'hekit', 't', 'y', 'likethedoggy']\n",
      "77 ['do', 'you', 's', 'eethekit', 'tyseeth', 'e', 'do', 'ggydo', 'you', 'like', 't', 'hekit', 't', 'y', 'likethedoggy']\n",
      "77 ['do', 'you', 's', 'eethekit', 'tyseeth', 'e', 'do', 'ggydo', 'you', 'like', 't', 'hekit', 't', 'y', 'likethedoggy']\n",
      "77 ['do', 'you', 's', 'eethekit', 'tyseeth', 'e', 'do', 'ggydo', 'you', 'like', 't', 'hekit', 't', 'y', 'likethedoggy']\n",
      "77 ['do', 'you', 's', 'eethekit', 'tyseeth', 'e', 'do', 'ggydo', 'you', 'like', 't', 'hekit', 't', 'y', 'likethedoggy']\n",
      "77 ['do', 'you', 's', 'eethekit', 'tyseeth', 'e', 'do', 'ggydo', 'you', 'like', 't', 'hekit', 't', 'y', 'likethedoggy']\n",
      "77 ['do', 'you', 's', 'eethekit', 'tyseeth', 'e', 'do', 'ggydo', 'you', 'like', 't', 'hekit', 't', 'y', 'likethedoggy']\n",
      "77 ['do', 'you', 's', 'eethekit', 'tyseeth', 'e', 'do', 'ggydo', 'you', 'like', 't', 'hekit', 't', 'y', 'likethedoggy']\n",
      "77 ['do', 'you', 's', 'eethekit', 'tyseeth', 'e', 'do', 'ggydo', 'you', 'like', 't', 'hekit', 't', 'y', 'likethedoggy']\n",
      "76 ['do', 'y', 'ou', 's', 'eethekittyse', 'ethe', 'd', 'oggydoy', 'oulikethekit', 'tylikethedoggy']\n",
      "74 ['do', 'y', 'ous', 'e', 'etheki', 'ttyse', 'e', 't', 'h', 'e', 'd', 'ogg', 'y', 'do', 'youli', 'k', 'etheki', 't', 't', 'ylikethed', 'ogg', 'y']\n",
      "74 ['do', 'y', 'ous', 'e', 'etheki', 'ttyse', 'e', 't', 'h', 'e', 'd', 'ogg', 'y', 'do', 'youli', 'k', 'etheki', 't', 't', 'ylikethed', 'ogg', 'y']\n",
      "74 ['do', 'y', 'ous', 'e', 'etheki', 'ttyse', 'e', 't', 'h', 'e', 'd', 'ogg', 'y', 'do', 'youli', 'k', 'etheki', 't', 't', 'ylikethed', 'ogg', 'y']\n",
      "74 ['do', 'y', 'ous', 'e', 'etheki', 'ttyse', 'e', 't', 'h', 'e', 'd', 'ogg', 'y', 'do', 'youli', 'k', 'etheki', 't', 't', 'ylikethed', 'ogg', 'y']\n",
      "74 ['do', 'y', 'ous', 'e', 'etheki', 'ttyse', 'e', 't', 'h', 'e', 'd', 'ogg', 'y', 'do', 'youli', 'k', 'etheki', 't', 't', 'ylikethed', 'ogg', 'y']\n",
      "74 ['do', 'y', 'ous', 'e', 'etheki', 'ttyse', 'e', 't', 'h', 'e', 'd', 'ogg', 'y', 'do', 'youli', 'k', 'etheki', 't', 't', 'ylikethed', 'ogg', 'y']\n",
      "74 ['do', 'y', 'ous', 'e', 'etheki', 'ttyse', 'e', 't', 'h', 'e', 'd', 'ogg', 'y', 'do', 'youli', 'k', 'etheki', 't', 't', 'ylikethed', 'ogg', 'y']\n",
      "74 ['do', 'y', 'ous', 'e', 'etheki', 'ttyse', 'e', 't', 'h', 'e', 'd', 'ogg', 'y', 'do', 'youli', 'k', 'etheki', 't', 't', 'ylikethed', 'ogg', 'y']\n",
      "74 ['do', 'y', 'ous', 'e', 'etheki', 'ttyse', 'e', 't', 'h', 'e', 'd', 'ogg', 'y', 'do', 'youli', 'k', 'etheki', 't', 't', 'ylikethed', 'ogg', 'y']\n",
      "74 ['do', 'y', 'ous', 'e', 'etheki', 'ttyse', 'e', 't', 'h', 'e', 'd', 'ogg', 'y', 'do', 'youli', 'k', 'etheki', 't', 't', 'ylikethed', 'ogg', 'y']\n",
      "74 ['do', 'y', 'ous', 'e', 'etheki', 'ttyse', 'e', 't', 'h', 'e', 'd', 'ogg', 'y', 'do', 'youli', 'k', 'etheki', 't', 't', 'ylikethed', 'ogg', 'y']\n",
      "74 ['do', 'y', 'ous', 'e', 'etheki', 'ttyse', 'e', 't', 'h', 'e', 'd', 'ogg', 'y', 'do', 'youli', 'k', 'etheki', 't', 't', 'ylikethed', 'ogg', 'y']\n",
      "74 ['do', 'y', 'ous', 'e', 'etheki', 'ttyse', 'e', 't', 'h', 'e', 'd', 'ogg', 'y', 'do', 'youli', 'k', 'etheki', 't', 't', 'ylikethed', 'ogg', 'y']\n",
      "74 ['do', 'y', 'ous', 'e', 'etheki', 'ttyse', 'e', 't', 'h', 'e', 'd', 'ogg', 'y', 'do', 'youli', 'k', 'etheki', 't', 't', 'ylikethed', 'ogg', 'y']\n",
      "74 ['do', 'y', 'ous', 'e', 'etheki', 'ttyse', 'e', 't', 'h', 'e', 'd', 'ogg', 'y', 'do', 'youli', 'k', 'etheki', 't', 't', 'ylikethed', 'ogg', 'y']\n",
      "74 ['do', 'y', 'ous', 'e', 'etheki', 'ttyse', 'e', 't', 'h', 'e', 'd', 'ogg', 'y', 'do', 'youli', 'k', 'etheki', 't', 't', 'ylikethed', 'ogg', 'y']\n",
      "74 ['do', 'y', 'ous', 'e', 'etheki', 'ttyse', 'e', 't', 'h', 'e', 'd', 'ogg', 'y', 'do', 'youli', 'k', 'etheki', 't', 't', 'ylikethed', 'ogg', 'y']\n",
      "74 ['do', 'y', 'ous', 'e', 'etheki', 'ttyse', 'e', 't', 'h', 'e', 'd', 'ogg', 'y', 'do', 'youli', 'k', 'etheki', 't', 't', 'ylikethed', 'ogg', 'y']\n",
      "74 ['do', 'y', 'ous', 'e', 'etheki', 'ttyse', 'e', 't', 'h', 'e', 'd', 'ogg', 'y', 'do', 'youli', 'k', 'etheki', 't', 't', 'ylikethed', 'ogg', 'y']\n",
      "74 ['do', 'y', 'ous', 'e', 'etheki', 'ttyse', 'e', 't', 'h', 'e', 'd', 'ogg', 'y', 'do', 'youli', 'k', 'etheki', 't', 't', 'ylikethed', 'ogg', 'y']\n",
      "73 ['doyous', 'e', 'etheki', 'ttyse', 'e', 'th', 'ed', 'ogg', 'ydo', 'youli', 'k', 'etheki', 'ttylikethed', 'ogg', 'y']\n",
      "67 ['doyous', 'e', 'etheki', 'ttys', 'e', 'ethed', 'ogg', 'ydo', 'y', 'ouli', 'k', 'etheki', 'ttyli', 'k', 'ethed', 'ogg', 'y']\n",
      "67 ['doyous', 'e', 'etheki', 'ttys', 'e', 'ethed', 'ogg', 'ydo', 'y', 'ouli', 'k', 'etheki', 'ttyli', 'k', 'ethed', 'ogg', 'y']\n",
      "67 ['doyous', 'e', 'etheki', 'ttys', 'e', 'ethed', 'ogg', 'ydo', 'y', 'ouli', 'k', 'etheki', 'ttyli', 'k', 'ethed', 'ogg', 'y']\n",
      "67 ['doyous', 'e', 'etheki', 'ttys', 'e', 'ethed', 'ogg', 'ydo', 'y', 'ouli', 'k', 'etheki', 'ttyli', 'k', 'ethed', 'ogg', 'y']\n",
      "67 ['doyous', 'e', 'etheki', 'ttys', 'e', 'ethed', 'ogg', 'ydo', 'y', 'ouli', 'k', 'etheki', 'ttyli', 'k', 'ethed', 'ogg', 'y']\n",
      "67 ['doyous', 'e', 'etheki', 'ttys', 'e', 'ethed', 'ogg', 'ydo', 'y', 'ouli', 'k', 'etheki', 'ttyli', 'k', 'ethed', 'ogg', 'y']\n",
      "67 ['doyous', 'e', 'etheki', 'ttys', 'e', 'ethed', 'ogg', 'ydo', 'y', 'ouli', 'k', 'etheki', 'ttyli', 'k', 'ethed', 'ogg', 'y']\n",
      "67 ['doyous', 'e', 'etheki', 'ttys', 'e', 'ethed', 'ogg', 'ydo', 'y', 'ouli', 'k', 'etheki', 'ttyli', 'k', 'ethed', 'ogg', 'y']\n",
      "67 ['doyous', 'e', 'etheki', 'ttys', 'e', 'ethed', 'ogg', 'ydo', 'y', 'ouli', 'k', 'etheki', 'ttyli', 'k', 'ethed', 'ogg', 'y']\n",
      "67 ['doyous', 'e', 'etheki', 'ttys', 'e', 'ethed', 'ogg', 'ydo', 'y', 'ouli', 'k', 'etheki', 'ttyli', 'k', 'ethed', 'ogg', 'y']\n",
      "67 ['doyous', 'e', 'etheki', 'ttys', 'e', 'ethed', 'ogg', 'ydo', 'y', 'ouli', 'k', 'etheki', 'ttyli', 'k', 'ethed', 'ogg', 'y']\n",
      "66 ['do', 'yous', 'e', 'etheki', 'ttys', 'e', 'ethedogg', 'ydo', 'y', 'ouli', 'k', 'etheki', 'ttyli', 'k', 'ethedogg', 'y']\n",
      "65 ['do', 'yous', 'e', 'etheki', 'ttys', 'e', 'ethedogg', 'y', 'do', 'y', 'ouli', 'k', 'etheki', 't', 'tyli', 'k', 'ethedogg', 'y']\n",
      "64 ['do', 'yous', 'e', 'etheki', 'tt', 'ys', 'e', 'ethedogg', 'y', 'do', 'y', 'oulik', 'etheki', 'tt', 'yli', 'k', 'ethedogg', 'y']\n",
      "62 ['do', 'yous', 'e', 'etheki', 'tt', 'ys', 'e', 'ethedogg', 'y', 'do', 'youlik', 'etheki', 'tt', 'ylik', 'ethedogg', 'y']\n",
      "62 ['do', 'yous', 'e', 'etheki', 'tt', 'ys', 'e', 'ethedogg', 'y', 'do', 'youlik', 'etheki', 'tt', 'ylik', 'ethedogg', 'y']\n",
      "62 ['do', 'yous', 'e', 'etheki', 'tt', 'ys', 'e', 'ethedogg', 'y', 'do', 'youlik', 'etheki', 'tt', 'ylik', 'ethedogg', 'y']\n",
      "60 ['do', 'youse', 'etheki', 'tt', 'yse', 'ethedogg', 'y', 'do', 'youlik', 'etheki', 'tt', 'y', 'lik', 'ethedogg', 'y']\n",
      "58 ['do', 'youse', 'etheki', 'tt', 'y', 'se', 'ethedogg', 'y', 'do', 'y', 'ou', 'lik', 'etheki', 'tt', 'y', 'lik', 'ethedogg', 'y']\n",
      "57 ['do', 'you', 'se', 'etheki', 'tt', 'y', 'se', 'ethedogg', 'y', 'do', 'y', 'ou', 'lik', 'etheki', 'tt', 'y', 'lik', 'ethedogg', 'y']\n",
      "53 ['do', 'you', 'se', 'etheki', 'tt', 'y', 'se', 'ethedogg', 'y', 'do', 'you', 'lik', 'etheki', 'tt', 'y', 'lik', 'ethedogg', 'y']\n",
      "53 ['do', 'you', 'se', 'etheki', 'tt', 'y', 'se', 'ethedogg', 'y', 'do', 'you', 'lik', 'etheki', 'tt', 'y', 'lik', 'ethedogg', 'y']\n",
      "53 ['do', 'you', 'se', 'etheki', 'tt', 'y', 'se', 'ethedogg', 'y', 'do', 'you', 'lik', 'etheki', 'tt', 'y', 'lik', 'ethedogg', 'y']\n",
      "53 ['do', 'you', 'se', 'etheki', 'tt', 'y', 'se', 'ethedogg', 'y', 'do', 'you', 'lik', 'etheki', 'tt', 'y', 'lik', 'ethedogg', 'y']\n",
      "50 ['do', 'you', 'se', 'ethekitt', 'y', 'se', 'ethedogg', 'y', 'do', 'you', 'lik', 'ethekitt', 'y', 'lik', 'ethedogg', 'y']\n",
      "47 ['doyou', 'se', 'ethekitt', 'y', 'se', 'ethedogg', 'y', 'doyou', 'lik', 'ethekitt', 'y', 'lik', 'ethedogg', 'y']\n",
      "47 ['doyou', 'se', 'ethekitt', 'y', 'se', 'ethedogg', 'y', 'doyou', 'lik', 'ethekitt', 'y', 'lik', 'ethedogg', 'y']\n",
      "46 ['doyou', 'se', 'ethekitt', 'y', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitt', 'y', 'lik', 'ethedoggy']\n",
      "46 ['doyou', 'se', 'ethekitt', 'y', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitt', 'y', 'lik', 'ethedoggy']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "43 ['doyou', 'se', 'ethekitty', 'se', 'ethedoggy', 'doyou', 'lik', 'ethekitty', 'lik', 'ethedoggy']\n",
      "\n",
      "0000101000000001010000000010000100100000000100100000000\n"
     ]
    }
   ],
   "source": [
    "bestseg = anneal(text, seg4)\n",
    "print(bestseg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the best segmentation includes \"words\" like ```thekitty```, since there's not enough evidence in the data to split this any further. \n",
    "\n",
    "This performs a *Non-Deterministic Search Using Simulated Annealing*, and the models begins searching with phrase segmentations only; it randomly perturbs the zeros and ones proportional to the \"temperature\"\n",
    "-  With each iteration the temperature is lowered and the perturbation of boundaries is reduced\n",
    "-  With enough data, it is possible to automatically segment text into words with a reasonable degree of accuracy\n",
    "-  Such methods can be applied to tokenization for writing systems that don't have any visual representation of word boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000100000100001000001000010000100000010000100000010000\n"
     ]
    }
   ],
   "source": [
    "bestest = bestseg\n",
    "print(bestest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting: An Exploration of Outputs\n",
    "\n",
    "Often we write a program to report a single data item, such as a particular element in a corpus that meets some complicated criterion, or a single summary statistic such as a word-count or the performance of a tagger. \n",
    "\n",
    "More often, we write a program to produce a structured result; for example, a tabulation of numbers or linguistic forms, or a reformatting of the original data. When the results to be presented are linguistic, textual output is usually the most natural choice. Here, we explore a variety of ways to format data.\n",
    "\n",
    "### Lists to Strings\n",
    "\n",
    "One of the simplest formatting conversions is using ```join()``` to take a list, and merge it into one single string. However, we don't haveto make it all one big string... whatever we want to join the letters on, we could use as the delimiter between the quotes in the join statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pleasureofmakingadaisy-chainwouldbeworththetroubleofgettingupandpickingthedaisies,whensuddenlyaWhiteRabbitwithpinkeyesranclosebyher.Therewasnothingsoveryremarkableinthat,nordidAlicethinkitsoverymuchoutof\n",
      "\n",
      "pleasure|of|making|a|daisy-chain|would|be|worth|the|trouble|of|getting|up|and|picking|the|daisies|,|when|suddenly|a|White|Rabbit|with|pink|eyes|ran|close|by|her|.|There|was|nothing|so|very|remarkable|in|that|,|nor|did|Alice|think|it|so|very|much|out|of\n"
     ]
    }
   ],
   "source": [
    "print(''.join(re_split_raw[100:150]))\n",
    "print()\n",
    "print('|'.join(re_split_raw[100:150]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Formats\n",
    "\n",
    "There are two ways to display the contents of an object. The ```print``` command yields Python's attempt to produce the most human-readable form of an object. The second method — *naming the variable at a prompt* — shows us a string that can be used to recreate this object. \n",
    "\n",
    "It is important to keep in mind that both of these are just strings, displayed for the benefit of us, the users. They do not give us any clue as to the actual internal representation of the object. There are many other ways to represent an object as a string of characters. This may be for the benefit of a human reader, or because we want to export our data to a particular file format for use in an external program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat -> 3; dog -> 4; snake -> 1; "
     ]
    }
   ],
   "source": [
    "# Formatted output containing a combination of variables and pre-specified strings\n",
    "# Using print() syntax\n",
    "fdist = nltk.FreqDist(['dog', 'cat', 'dog', 'cat', 'dog', 'snake', 'dog', 'cat'])\n",
    "for word in sorted(fdist):\n",
    "    print(word, '->', fdist[word], end='; ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another solution, which we used quitely in some example above, is to use the ```format()``` method, which has extensive customization syntax available. \n",
    "\n",
    "The curly brackets '{}' mark the presence of a replacement field: this acts as a placeholder for the string values of objects that are passed to the ```format()``` method. We can embed occurrences of '{}' inside a string, then replacet them with strings by calling ```format()``` with appropriate arguments. A string containing replacement fields is called a format string.\n",
    "-  We can have any number of placeholders, but the str.format method must be called with exactly the same number of arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat->3; dog->4; snake->1; "
     ]
    }
   ],
   "source": [
    "# Formatted output containing a combination of variables and pre-specified strings\n",
    "# Using format() function\n",
    "for word in sorted(fdist):\n",
    "    print('{0}->{1};'.format(word, fdist[word]), end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lee wants a sandwich right now\n",
      "Lee wants a pizza right now\n",
      "Lee wants a hot potato right now\n"
     ]
    }
   ],
   "source": [
    "# Iterative format() template fill\n",
    "template = 'Lee wants a {} right now'\n",
    "menu = ['sandwich', 'pizza', 'hot potato']\n",
    "\n",
    "for snack in menu:\n",
    "    print(template.format(snack))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alignment\n",
    "\n",
    "Things get be hard to make sense of if they aren't lined up on the screen properly, so you can take advantage of ```format``` to align you outputs with padding using a : between the curly brackets, followed by an integer representing spaces in padding. This integer can be preceded with a > or < symbol to specify left or right jsutification on top of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    42Hi\n",
      "42    Hi\n"
     ]
    }
   ],
   "source": [
    "print('{:6}Hi'.format(42))\n",
    "print('{:<6}Hi' .format(42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision\n",
    "\n",
    "Other special formatting can be utilized to handle precision or recognition of various number types. The string formatting is smart enough to know that if you include a '%' in your format specification, then you want to represent the value as a percentage, so you don't need to do extra multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1415926536\n",
      "2e+10\n",
      "Accuracy for 9000 words: 94.4444%\n"
     ]
    }
   ],
   "source": [
    "# Pi to 10 digits of Precision\n",
    "import math\n",
    "print('{:.10f}'.format(math.pi))\n",
    "print('{:0g}'.format(2e10))\n",
    "\n",
    "count, total = 8500, 9000\n",
    "print(\"Accuracy for {} words: {:.4%}\".format(total, count / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabulating Data\n",
    "\n",
    "We saw what a tabulating looks like from a conditional frequency distribution, but it's a good idea to have an algorithm to produce one with word distributions across topics for our toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tabulate Word Distriubtion Across Topics\n",
    "def tabulateFile(name, cfdist, words, topics):\n",
    "    # Open file for writing output\n",
    "    output = open('{0}_tabulate.txt'.format(name), 'w')\n",
    "    print('{:16}'.format('Topic')\n",
    "          , end=' ', file=output)            # column headings\n",
    "    \n",
    "    for word in words:\n",
    "        print('{:>6}'.format(word), end=' '\n",
    "              , file=output)                 # print to output file\n",
    "    print(file=output)\n",
    "    \n",
    "    for topic in topics:\n",
    "        print('{:16}'.format(topic)\n",
    "              , end=' ', file=output)        # row heading\n",
    "        for word in words:                   # for each word\n",
    "            print('{:6}'.format(cfdist[topic][word])\n",
    "                  , end=' ', file=output)    # print table cell\n",
    "        print(file=output)\n",
    "    output.close()\n",
    "    \n",
    "    # Display printed results from file\n",
    "    with open('{0}_tabulate.txt'.format(name), 'r') as t:\n",
    "        print(t.read())\n",
    "\n",
    "# For comparison, we could also format this in a manner that is dataframe ready\n",
    "def tabulateReturn(cfdist, words, topics):\n",
    "    tabbed = []\n",
    "    # Row Headings\n",
    "    for topic in topics:\n",
    "        row = []\n",
    "        row.append('{:<16}'.format(topic))         # Uses topics as row names\n",
    "        # For each word\n",
    "        for word in words:                         # Fill table with\n",
    "            row.append('{:5}'                      # word frequency in topic\n",
    "                       .format(cfdist[topic][word]))  \n",
    "        tabbed.append(row)\n",
    "    return tabbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic               can  could    may  might   must   will \n",
      "news                 93     86     66     38     50    389 \n",
      "religion             82     59     78     12     54     71 \n",
      "hobbies             268     58    131     22     83    264 \n",
      "science_fiction      16     49      4     12      8     16 \n",
      "romance              74    193     11     51     45     43 \n",
      "humor                16     30      8      8      9     13 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cfd = nltk.ConditionalFreqDist(\n",
    "           (genre, word)\n",
    "           for genre in brown.categories()\n",
    "           for word in brown.words(categories=genre))\n",
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "\n",
    "\n",
    "tabulateFile('brown', cfd, modals, genres)\n",
    "\n",
    "df_brown = pd.DataFrame(tabulateReturn(cfd, modals, genres)\n",
    "                        , columns=('Topic', 'can', 'could', 'may', 'might', 'must', 'will'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>can</th>\n",
       "      <th>could</th>\n",
       "      <th>may</th>\n",
       "      <th>might</th>\n",
       "      <th>must</th>\n",
       "      <th>will</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>news</td>\n",
       "      <td>93</td>\n",
       "      <td>86</td>\n",
       "      <td>66</td>\n",
       "      <td>38</td>\n",
       "      <td>50</td>\n",
       "      <td>389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>religion</td>\n",
       "      <td>82</td>\n",
       "      <td>59</td>\n",
       "      <td>78</td>\n",
       "      <td>12</td>\n",
       "      <td>54</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hobbies</td>\n",
       "      <td>268</td>\n",
       "      <td>58</td>\n",
       "      <td>131</td>\n",
       "      <td>22</td>\n",
       "      <td>83</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>science_fiction</td>\n",
       "      <td>16</td>\n",
       "      <td>49</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>romance</td>\n",
       "      <td>74</td>\n",
       "      <td>193</td>\n",
       "      <td>11</td>\n",
       "      <td>51</td>\n",
       "      <td>45</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>humor</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Topic    can  could    may  might   must   will\n",
       "0  news                 93     86     66     38     50    389\n",
       "1  religion             82     59     78     12     54     71\n",
       "2  hobbies             268     58    131     22     83    264\n",
       "3  science_fiction      16     49      4     12      8     16\n",
       "4  romance              74    193     11     51     45     43\n",
       "5  humor                16     30      8      8      9     13"
      ]
     },
     "execution_count": 764,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Wrapping \n",
    "\n",
    "When the output of our program is text-like, instead of tabular, it will usually be necessary to wrap it so that it can be displayed conveniently. We can take care of line wrapping with the help of Python's ```textwrap``` module. For maximum clarity we will separate each step onto its own line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After (5), all (3), is (2), said (4), and (3), done (4), , (1), more (4), is (2), said (4), than (4), done (4), . (1), "
     ]
    }
   ],
   "source": [
    "saying = ['After', 'all', 'is', 'said', 'and', 'done', ',',\n",
    "          'more', 'is', 'said', 'than', 'done', '.']\n",
    "\n",
    "# Prints words, followed by their length in order\n",
    "for word in saying:\n",
    "    print(word, '(' + str(len(word)) + '),', end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After (5), all (3), is (2), said (4), and (3), done (4), , (1),\n",
      "more (4), is (2), said (4), than (4), done (4), . (1),\n"
     ]
    }
   ],
   "source": [
    "from textwrap import fill\n",
    "\n",
    "format = '%s_(%d),'\n",
    "pieces = [format % (word, len(word)) for word in saying]\n",
    "output = ' '.join(pieces)\n",
    "wrapped = fill(output)\n",
    "print(wrapped.replace('_', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colourless\n"
     ]
    }
   ],
   "source": [
    "# Write a statement that changes s to \"colourless\" \n",
    "# using only the slice and concatenation operations\n",
    "s = 'colorless'\n",
    "s = s[:4] + 'u' + s[4:]\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dish\n",
      "run\n",
      "nation\n",
      "un\n",
      "pre\n"
     ]
    }
   ],
   "source": [
    "# Use slice notation to remove the affixes from these words:\n",
    "words = ['dishes', 'running', 'nationality', 'undo', 'preheat']\n",
    "\n",
    "print(words[0][:-2])\n",
    "print(words[1][:-4])\n",
    "print(words[2][:-5])\n",
    "print(words[3][:-2])\n",
    "print(words[4][:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nohtyP ytnoM\n"
     ]
    }
   ],
   "source": [
    "# What happens if you ask the interpreter to evaluate monty[::-1]?\n",
    "monty = 'Monty Python'\n",
    "\n",
    "# Reverses a string\n",
    "print(monty[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write a utility function that takes a URL as its argument, \n",
    "# and returns the contents of the URL, with all HTML markup removed\n",
    "\n",
    "def textFromHTML(url):\n",
    "    from urllib import request \n",
    "    from bs4 import BeautifulSoup\n",
    "    \n",
    "    html = request.urlopen(url).read().decode('utf8')\n",
    "    html_raw = BeautifulSoup(html, \"lxml\")\n",
    "    tokens = word_tokenize(html_raw.get_text())\n",
    "    html_text = nltk.Text([w.lower() for w in tokens if w.isalpha() and len(w) > 3])\n",
    "    return html_text.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['contoso',\n",
       " 'wikipedia',\n",
       " 'function',\n",
       " 'wgcanonicalnamespace',\n",
       " 'wgcanonicalspecialpagename',\n",
       " 'false',\n",
       " 'wgnamespacenumber',\n",
       " 'wgpagename',\n",
       " 'contoso',\n",
       " 'wgtitle',\n",
       " 'contoso',\n",
       " 'wgcurrevisionid',\n",
       " 'wgrevisionid',\n",
       " 'wgarticleid',\n",
       " 'wgisarticle']"
      ]
     },
     "execution_count": 796,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFromHTML('https://en.wikipedia.org/wiki/Contoso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'how': 6,\n",
       "          'what': 27,\n",
       "          'when': 12,\n",
       "          'where': 6,\n",
       "          'which': 10,\n",
       "          'while': 2,\n",
       "          'who': 21,\n",
       "          'whom': 4,\n",
       "          'whose': 2,\n",
       "          'why': 3})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Read in some text from a corpus, tokenize it, and print the list of all wh-word types that occur.\n",
    "#\n",
    "# (wh-words in English are used in questions, relative clauses and exclamations: \n",
    "# who, where, which, what, why, and so on (*and also including how)\n",
    "\n",
    "q = ['what', 'when', 'where', 'while', 'who', 'why', 'which', 'whom', 'whose', 'how']\n",
    "qfd = nltk.FreqDist([word for word in monty if word in q])\n",
    "display(qfd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ca', \"n't\"), ('ca', \"n't\"), ('ca', \"n't\"), ('wo', \"n't\"), ('had', \"n't\"), ('ca', \"n't\"), ('could', \"n't\"), ('ca', \"n't\"), ('could', \"n't\"), ('ca', \"n't\"), ('could', \"n't\"), ('o', \"n't\"), ('ca', \"n't\"), ('could', \"n't\"), ('ca', \"n't\"), ('could', \"n't\"), ('did', \"n't\"), ('ca', \"n't\"), ('ca', \"n't\"), ('do', \"n't\"), ('did', \"n't\"), ('could', \"n't\"), ('was', \"n't\"), ('could', \"n't\"), ('do', \"n't\"), ('wo', \"n't\"), ('ca', \"n't\"), ('do', \"n't\"), ('do', \"n't\"), ('o', \"n't\"), ('dare', \"n't\"), ('dare', \"n't\"), ('is', \"n't\"), ('did', \"n't\"), ('did', \"n't\"), ('did', \"n't\"), ('was', \"n't\"), ('was', \"n't\"), ('was', \"n't\"), ('was', \"n't\"), ('did', \"n't\"), ('could', \"n't\"), ('ca', \"n't\"), ('a', \"n't\"), ('could', \"n't\"), ('did', \"n't\"), ('do', \"n't\"), ('do', \"n't\"), ('could', \"n't\"), ('was', \"n't\"), ('ai', \"n't\"), ('ai', \"n't\"), ('o', \"n't\"), ('ai', \"n't\"), ('ai', \"n't\"), ('ai', \"n't\"), ('ai', \"n't\"), ('do', \"n't\"), ('did', \"n't\"), ('could', \"n't\"), ('did', \"n't\"), ('do', \"n't\"), ('do', \"n't\"), ('did', \"n't\"), ('could', \"n't\"), ('ca', \"n't\"), ('does', \"n't\"), ('do', \"n't\"), ('do', \"n't\"), ('o', \"n't\"), ('o', \"n't\"), ('is', \"n't\"), ('do', \"n't\"), ('do', \"n't\"), ('does', \"n't\"), ('is', \"n't\"), ('do', \"n't\"), ('do', \"n't\"), ('a', \"n't\"), ('do', \"n't\"), ('do', \"n't\"), ('do', \"n't\"), ('do', \"n't\"), ('do', \"n't\"), ('do', \"n't\"), ('ca', \"n't\"), ('could', \"n't\"), ('would', \"n't\"), ('do', \"n't\"), ('do', \"n't\"), ('do', \"n't\"), ('do', \"n't\"), ('do', \"n't\"), ('do', \"n't\"), ('did', \"n't\"), ('s', \"n't\"), ('do', \"n't\"), ('wo', \"n't\"), ('could', \"n't\"), ('is', \"n't\")]\n"
     ]
    }
   ],
   "source": [
    "# Are you able to write a regular expression to tokenize text in such a way \n",
    "# that the word don't is tokenized into do and n't? \n",
    "# Explain why this regular expression won't work: «n't|\\w+»\n",
    "\n",
    "apostrophe_match = '[a-z]+n\\'t'\n",
    "\n",
    "conjunctions = re.findall(apostrophe_match, text)\n",
    "\n",
    "tokens = [(word[:-3], word[-3:]) for word in conjunctions]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pig Latin is a simple transformation of English text. \n",
    "1. Write a function to convert a word to Pig Latin.\n",
    "2. Write code that converts text, instead of individual words.\n",
    "3. Extend it further to preserve capitalization, to keep qu together (i.e. so that quiet becomes ietquay), and to detect when y is used as a consonant (e.g. yellow) vs a vowel (e.g.  style)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Each word of the text is converted as follows: \n",
    "# Move any consonant (or consonant cluster) that appears at the start \n",
    "# of the word to the end, then append ay, \n",
    "# e.g. string → ingstray, idle → idleay\n",
    "\n",
    "def convertToPigLatin(text):\n",
    "    out = []\n",
    "    for w in text: # Doesn't yet support QU or Y as Vowels\n",
    "        split = re.sub('^[bfdtlmnrpvcgjkqsxz]+|qu+', '' , w, flags=re.I)\n",
    "        front = re.match('^[bfdtlmnrpvcgjkqsxz]|qu+', w, flags=re.I)\n",
    "        if front:\n",
    "            piggify = str.format('{0}{1}ay', split, front.group(0))\n",
    "        else: piggify = str.format('{0}ay', split)\n",
    "        out.append(piggify)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ewlynay',\n",
       " 'ormedfay',\n",
       " 'andbay',\n",
       " 'ideasay',\n",
       " 'areay',\n",
       " 'inexpressibleay',\n",
       " 'inay',\n",
       " 'anay',\n",
       " 'infuriatingay',\n",
       " 'wayay']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convertToPigLatin(bland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['newly',\n",
       " 'formed',\n",
       " 'bland',\n",
       " 'ideas',\n",
       " 'are',\n",
       " 'inexpressible',\n",
       " 'in',\n",
       " 'an',\n",
       " 'infuriating',\n",
       " 'way']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word for word in bland]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define a variable ```silly``` to contain the string: ```'newly formed bland ideas are inexpressible in an infuriating\n",
    "way'```** \n",
    "...This happens to be the legitimate interpretation that bilingual English-Spanish speakers can assign to Chomsky's famous nonsense phrase, ```colorless green ideas sleep furiously```, according to Wikipedia. \n",
    "Now write code to perform the following tasks:\n",
    "\n",
    "1. Split silly into a list of strings, one per word, using Python's split() operation, and save this to a variable called bland.\n",
    "2. Extract the second letter of each word in silly and join them into a string, to get 'eoldrnnnna'.\n",
    "3. Combine the words in bland back into a single string, using join(). Make sure the words in the resulting string are separated with whitespace.\n",
    "4. Print the words of silly in alphabetical order, one per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['newly', 'formed', 'bland', 'ideas', 'are', 'inexpressible', 'in', 'an', 'infuriating', 'way']\n",
      "eoldrnnnna\n",
      "newly formed bland ideas are inexpressible in an infuriating way\n",
      "an\n",
      "are\n",
      "bland\n",
      "formed\n",
      "ideas\n",
      "in\n",
      "inexpressible\n",
      "infuriating\n",
      "newly\n",
      "way\n"
     ]
    }
   ],
   "source": [
    "silly = 'newly formed bland ideas are inexpressible in an infuriating way'\n",
    "bland = silly.split(' ') #1 Check\n",
    "print(bland)\n",
    "\n",
    "s = ''\n",
    "for word in bland:\n",
    "    s += word[1]         #2 Check \n",
    "print(s)\n",
    "    \n",
    "join = ' '.join(bland)   #3 Check\n",
    "print(join)\n",
    "\n",
    "for word in sorted(bland):\n",
    "    print(word)          #4 Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soundex Algorithm\n",
    "\n",
    "According to Wikipedia, Soundex is a phonetic algorithm for **indexing names by sound**, as pronounced in English that has been around since the 1930s. The goal is for homophones to be encoded to the same representation so that they can be matched despite minor differences in spelling. The algorithm mainly encodes consonants; a vowel will not be encoded unless it is the first letter. \n",
    "\n",
    "The Soundex code for a name consists of a letter followed by three numerical digits: the letter is the first letter of the name, and the digits encode the remaining consonants.\n",
    "\n",
    "The correct value can be found as follows:\n",
    "\n",
    "1. Retain the first letter of the name and drop all other occurrences of a, e, i, o, u, y, h, w.\n",
    "2. Replace consonants with digits as follows (after the first letter):\n",
    "  - b, f, p, v → 1\n",
    "  - c, g, j, k, q, s, x, z → 2\n",
    "  - d, t → 3\n",
    "  - l → 4\n",
    "  - m, n → 5\n",
    "  - r → 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implementation of Soundex Name Indexing Algorithm in Python\n",
    "def soundex(name):\n",
    "    import re\n",
    "    \n",
    "    output = name[0].upper()               # Retain the first letter of the name \n",
    "    name = re.sub('[hw]'                   # Drop all occurrences of h, w\n",
    "                  , '', name, flags=re.I)  # flags=re.I |ignore case\n",
    "    \n",
    "    # Replace consonants with their respective digits\n",
    "    name = re.sub('[bfpv]+', '1', name, flags=re.I)\n",
    "    name = re.sub('[cgjkqsxz]+', '2', name, flags=re.I)\n",
    "    name = re.sub('[dt]+', '3', name, flags=re.I)\n",
    "    name = re.sub('l+', '4', name, flags=re.I)\n",
    "    name = re.sub('[mn]+', '5', name, flags=re.I)\n",
    "    name = re.sub('r+', '6', name, flags=re.I)\n",
    "    \n",
    "    # Chop off the first from the transformed string\n",
    "    name = name[1:]\n",
    "    # Drop all occurrences of a, e, i, o, u, y\n",
    "    name = re.sub('[aeiouy]', '', name, flags=re.I)\n",
    "    # Append first 3 digits of transformed text to output\n",
    "    output += name[:3]\n",
    "    \n",
    "    # Fill with 0s in the event there are < 4 chars\n",
    "    if len(output) < 4:\n",
    "        output += '0'*(4-len(output))\n",
    "    return str(output)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W350\n",
      "S315\n",
      "M200\n",
      "G460\n",
      "N300\n"
     ]
    }
   ],
   "source": [
    "print(soundex('Whitney'))\n",
    "print(soundex('Stephen'))\n",
    "print(soundex('Mike'))\n",
    "print(soundex('Gloria'))\n",
    "print(soundex('Nat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Identification \n",
    "\n",
    "With the help of a multilingual corpus such as the Universal Declaration of Human Rights Corpus (nltk.corpus.udhr), and NLTK's frequency distribution and rank correlation functionality (nltk.FreqDist, nltk.spearman_correlation), develop a system that guesses the language of a previously unseen text\n",
    "\n",
    "**Conclusions:** After making this, it's functional, but doesn't seem to do a great job when presented with more than a handful of languages at a time, espcially when more than one of those languages have very similar roots (ie German and Dutch or Spanish and Portuguese). It/s also very sensitive to length and use of characters in the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ingests specified language corpuses from UDHR (fileids):\n",
    "# Returns a tokenized frequency distribution for each to use as\n",
    "# a base from which to calculate correlation to the unseen text\n",
    "\n",
    "from nltk import *\n",
    "from nltk.corpus import udhr\n",
    "from nltk import spearman_correlation\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "def indexlangs():   \n",
    "    # Gather list of available language corpuses\n",
    "    langs = udhr.fileids()\n",
    "    \n",
    "    # Match Latin1 encoded languages\n",
    "    L = ['French_Francais-Latin1', 'Spanish-Latin1', 'German_Deutsch-Latin1' , 'English-Latin1']\n",
    "    langs = [fileid for fileid in langs if fileid in L]\n",
    "    langs = list(enumerate(langs)) # Easy-nav index of languages\n",
    "    \n",
    "    udhr_corpus = []\n",
    "    for lang in langs:\n",
    "        udhr_corpus.append(processText(udhr.words(lang[1])))\n",
    "    \n",
    "    fd_lang = []\n",
    "    rank_lang = []\n",
    "    for lang in udhr_corpus:\n",
    "        fq, rk = freqDistRank(lang)\n",
    "        fd_lang.append(fq)\n",
    "        rank_lang.append(rk)\n",
    "    return langs, rank_lang\n",
    "\n",
    "def indexUnseenText(text):\n",
    "    # Ingests unseen texts, returns tokenized frequency distribution\n",
    "    unseen = processText(text)\n",
    "    \n",
    "    # Get FreqDist, and Ranks for text\n",
    "    fd_unseen, rank_unseen = freqDistRank(text)\n",
    "    return unseen, rank_unseen\n",
    "\n",
    "def processText(text):\n",
    "    # Tokenize each letter of each word, in each language\n",
    "    text = [list(word.lower()) for word in text if str(word).isalpha()]\n",
    "    \n",
    "    # Transpose character lists of each word, creating one \n",
    "    # long tokenized list of characters for each language\n",
    "    text = [char for word in text for char in word] \n",
    "    return text\n",
    "\n",
    "def freqDistRank(text):\n",
    "    # Returns FreqDist then converts that distribution\n",
    "    # to a Spearman ranking by score\n",
    "    \n",
    "    dist = FreqDist(text)\n",
    "    rank = list(ranks_from_sequence(dist))\n",
    "    return dist, rank\n",
    "\n",
    "def correlation(unseen_ranks, lang_ranks):\n",
    "    # Calculates Spearman correlation between unseen text\n",
    "    # ranks and the ranks of the indexed langs\n",
    "    sc = []\n",
    "    for lang in lang_ranks:\n",
    "        cor = spearman_correlation(lang, unseen_ranks)\n",
    "        sc.append(cor)\n",
    "    return sc\n",
    "\n",
    "def matchLanguage(text):\n",
    "    # Calls all the functions to correlate unseen text to languages\n",
    "    langs, lang_ranks = indexlangs()\n",
    "    unseen, unseen_ranks = indexUnseenText(text)\n",
    "    cor = correlation(unseen_ranks, lang_ranks)\n",
    "    \n",
    "    # Compiles languages and spearman correlations into one list\n",
    "    zippy = list(zip(langs, cor))\n",
    "    \n",
    "    # Sorts the lists, and returns the highest value correlation\n",
    "    zippy = sorted(zippy, key=lambda x: x[1]).pop()\n",
    "\n",
    "    return zippy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 'German_Deutsch-Latin1'), -0.6401098901098901)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((1, 'French_Francais-Latin1'), 0.08970588235294119)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((0, 'English-Latin1'), -0.6373626373626373)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((0, 'English-Latin1'), -0.5053571428571428)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spanish = \"Dime con quién andas, y te diré quién eres\"\n",
    "french = \"Chacun voit midi à sa porte\"\n",
    "german = \"Des Teufels liebstes Möbelstück ist die lange Bank\"\n",
    "english = \"If you are going through hell, keep going.\"\n",
    "\n",
    "display(matchLanguage(spanish))\n",
    "display(matchLanguage(french))\n",
    "display(matchLanguage(german))\n",
    "display(matchLanguage(english))\n",
    "# 50% success rate... blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Whoops, I wrote my own Frequency Distribution function\n",
    "\n",
    "#freqs = {}\n",
    "#for lang in fids:\n",
    "#    freqs[lang] = {}\n",
    "#    for word in udhr.words(lang):\n",
    "#        if word.isalpha():\n",
    "#            for char in word.lower():\n",
    "#                if char in freqs[lang]:\n",
    "#                    freqs[lang][char] += 1\n",
    "#                else: freqs[lang][char] = 1\n",
    "#display(freqs)           "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
